{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class term():\n",
    "    joint_ids = ['FL_hip_joint', 'FR_hip_joint', 'RL_hip_joint', 'RR_hip_joint', 'FL_thigh_joint', 'FR_thigh_joint', 'RL_thigh_joint', 'RR_thigh_joint', 'FL_calf_joint', 'FR_calf_joint', 'RL_calf_joint', 'RR_calf_joint']#['RR_hip', 'RR_joint', 'RR_calf']\n",
    "    num_envs = 64\n",
    "    _num_legs = 4\n",
    "    _prevision_horizon = 10\n",
    "\n",
    "    def __init__(self):\n",
    "        self._num_joints = len(self.joint_ids)\n",
    "        self.f = torch.zeros(self.num_envs, self._num_legs)\n",
    "        self.d = torch.zeros(self.num_envs, self._num_legs)\n",
    "        self.p = torch.zeros(self.num_envs, self._num_legs, self._prevision_horizon)\n",
    "        self.F = torch.zeros(self.num_envs, self._num_legs, self._prevision_horizon)\n",
    "        self.z = [self.f, self.d, self.p, self.F]\n",
    "\n",
    "        # create tensors for raw and processed actions\n",
    "        self._raw_actions = torch.zeros(self.num_envs, self.action_dim2)\n",
    "        self._processed_actions = torch.zeros_like(self.raw_actions)\n",
    "\n",
    "    @property\n",
    "    def action_dim(self) -> int:\n",
    "        return self._num_joints\n",
    "    \n",
    "    @property\n",
    "    def action_dim2(self) -> int:\n",
    "        return self.f.shape[1:].numel() + self.d.shape[1:].numel() + self.p.shape[1:].numel() + self.F.shape[1:].numel()\n",
    "    \n",
    "    @property\n",
    "    def action_dim3(self) -> int:\n",
    "        return sum(variable.shape[1:].numel() for variable in self.z)\n",
    "    \n",
    "    @property\n",
    "    def raw_actions(self) -> torch.Tensor:\n",
    "        return self._raw_actions\n",
    "\n",
    "    @property\n",
    "    def processed_actions(self) -> torch.Tensor:\n",
    "        return self._processed_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "term1 = term()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term1.action_dim2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term1.action_dim3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 88])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term1.raw_actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term1.z[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term1.num_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor : tensor([[[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         ...,\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         ...,\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         ...,\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         ...,\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         ...,\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         ...,\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "shape : 24\n",
      "torch.Size([1536])\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros(term1.num_envs, term1.action_dim,2)\n",
    "print('tensor :',a)\n",
    "print('shape :', a.shape[1:].numel())\n",
    "\n",
    "print(a.flatten().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'omni.physics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01momni\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01misaac\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01morbit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AssetBase\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01momni\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01misaac\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01morbit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marticulation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Articulation\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01momni\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01misaac\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01morbit_tasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlocomotion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_based\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_based_env_cfg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LocomotionModelBasedEnvCfg\n",
      "File \u001b[0;32m~/Documents/dls_orbit_bat_private/source/extensions/omni.isaac.orbit/omni/isaac/orbit/assets/__init__.py:41\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) 2022-2024, The ORBIT Project Developers.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# SPDX-License-Identifier: BSD-3-Clause\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"Sub-package for different assets, such as rigid objects and articulations.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03mAn asset is a physical object that can be spawned in the simulation. The class handles both\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03mthe corresponding actuator torques.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marticulation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Articulation, ArticulationCfg, ArticulationData\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masset_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AssetBase\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masset_base_cfg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AssetBaseCfg\n",
      "File \u001b[0;32m~/Documents/dls_orbit_bat_private/source/extensions/omni.isaac.orbit/omni/isaac/orbit/assets/articulation/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) 2022-2024, The ORBIT Project Developers.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# SPDX-License-Identifier: BSD-3-Clause\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"Sub-module for rigid articulated assets.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marticulation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Articulation\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marticulation_cfg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArticulationCfg\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marticulation_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArticulationData\n",
      "File \u001b[0;32m~/Documents/dls_orbit_bat_private/source/extensions/omni.isaac.orbit/omni/isaac/orbit/assets/articulation/articulation.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcarb\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01momni\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mphysics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mphysx\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01momni\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01misaac\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArticulationActions\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpxr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UsdPhysics\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'omni.physics'"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from omni.isaac.orbit.assets import AssetBase\n",
    "from omni.isaac.orbit.assets.articulation import Articulation\n",
    "from omni.isaac.orbit_tasks.locomotion.model_based.model_based_env_cfg import LocomotionModelBasedEnvCfg\n",
    "from omni.isaac.orbit_tasks.locomotion.model_based.config.unitree_aliengo.aliengo_base_env_cfg import UnitreeAliengoBaseEnvCfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "seed = 42\n",
    "key = jax.random.key(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available devices: 1\n",
      "current device: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3060'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'available devices: {torch.cuda.device_count()}')\n",
    "print(f'current device: { torch.cuda.current_device()}')\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape :  torch.Size([64, 12])\n",
      "device :  cuda:0\n"
     ]
    }
   ],
   "source": [
    "output_torques = (torch.rand(term1.num_envs, term1._num_joints, device='cuda') * 80) - 40\n",
    "print('shape : ',output_torques.shape)\n",
    "print('device : ',output_torques.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_torques_jax = jax.random.normal(key=key, shape=output_torques.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Torch ---\n",
      "Shape :  torch.Size([64, 12])\n",
      "Type :  torch.FloatTensor\n",
      "Type :  <class 'torch.Tensor'>\n",
      "\n",
      "--- Jax ---\n",
      "Shape :  (64, 12)\n",
      "Type :  <class 'jaxlib.xla_extension.ArrayImpl'>\n"
     ]
    }
   ],
   "source": [
    "print('--- Torch ---')\n",
    "print('Shape : ', output_torques.shape)\n",
    "print('Type : ', output_torques.type())\n",
    "print('Type : ', type(output_torques))\n",
    "\n",
    "print('')\n",
    "print('--- Jax ---')\n",
    "print('Shape : ', output_torques_jax.shape)\n",
    "print('Type : ', type(output_torques_jax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1.0063325, dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_torques_jax.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.dlpack\n",
    "import torch\n",
    "import torch.utils.dlpack\n",
    "\n",
    "def jax_to_torch(x):\n",
    "    return torch.utils.dlpack.from_dlpack(jax.dlpack.to_dlpack(x))\n",
    "def torch_to_jax(x):\n",
    "    return jax.dlpack.from_dlpack(torch.utils.dlpack.to_dlpack(x))\n",
    "\n",
    "a = torch.tensor([1,2,3]).cuda()\n",
    "a_jax = torch_to_jax(a)\n",
    "print(a_jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{cuda(id=0)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape :  torch.Size([64, 12])\n",
      "device :  cuda:0\n",
      "Type :  <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "output_torques = (torch.rand(term1.num_envs, term1._num_joints, device='cuda') * 80) - 40\n",
    "print('shape : ',output_torques.shape)\n",
    "print('device : ',output_torques.device)\n",
    "print('Type : ', type(output_torques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape :  (64, 12)\n",
      "device :  {cuda(id=0)}\n",
      "Type :  <class 'jaxlib.xla_extension.ArrayImpl'>\n"
     ]
    }
   ],
   "source": [
    "output_torques_jax = torch_to_jax(output_torques)\n",
    "print('Shape : ', output_torques_jax.shape)\n",
    "print('device : ',output_torques_jax.devices())\n",
    "print('Type : ', type(output_torques_jax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alo() -> tuple[int, int, str]:\n",
    "    a = 2\n",
    "    b = 3\n",
    "    c = 4\n",
    "    return a, b, str(c)\n",
    "\n",
    "def alo2():\n",
    "    a = 2\n",
    "    b = 3\n",
    "    c = 4\n",
    "    return a, b, str(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, '4')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, '4')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alo2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "print(type(alo()))\n",
    "print(type(alo2()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tuple[2,3,4]\n",
    "alo()\n",
    "\n",
    "d, f, e = alo()\n",
    "type(alo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.bool\n",
      "tensor([[0.5317, 0.4781, 0.3271],\n",
      "        [0.3938, 0.3433, 0.9002]])\n",
      "tensor([[ True, False,  True],\n",
      "        [False,  True, False]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5317, 0.0000, 0.3271],\n",
       "        [0.0000, 0.3433, 0.0000]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([0, 1.21, 2])\n",
    "b = torch.tensor([True, True, False])\n",
    "\n",
    "shape = [2,3]\n",
    "a = torch.rand(shape)\n",
    "b = torch.empty(shape, dtype=torch.bool).bernoulli(0.5)\n",
    "\n",
    "\n",
    "print(a.dtype)\n",
    "print(b.dtype)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[5, 4, 3]' is invalid for input of size 20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m number_of_joint_per_leg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Modify the tensor to shape (batch_size, num_legs, number_of_joint_per_leg)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m modified_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_joint_per_leg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Check the shape of the modified tensor\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModified tensor shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, modified_tensor\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[5, 4, 3]' is invalid for input of size 20"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you have a tensor of shape (batch_size, num_legs)\n",
    "tensor = torch.randn(5, 4)  # Example tensor with shape (5, 4)\n",
    "\n",
    "# Define the number of joints per leg\n",
    "number_of_joint_per_leg = 3\n",
    "\n",
    "# Modify the tensor to shape (batch_size, num_legs, number_of_joint_per_leg)\n",
    "modified_tensor = torch.reshape(tensor, (tensor.shape[0], tensor.shape[1], number_of_joint_per_leg))\n",
    "\n",
    "# Check the shape of the modified tensor\n",
    "print(\"Modified tensor shape:\", modified_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.00016944000124931335 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# Create some tensors for demonstration\n",
    "T_shape = [4096,4,3]\n",
    "c_shape = [4096,4]\n",
    "T_1 = torch.rand(T_shape).cuda()\n",
    "T_2 = torch.rand(T_shape).cuda()\n",
    "c = torch.empty(c_shape, dtype=torch.bool).bernoulli(0.5).cuda()\n",
    "\n",
    "\n",
    "# Create CUDA events\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "# Record start event\n",
    "start_event.record()\n",
    "\n",
    "# Example operation (e.g., matrix multiplication)\n",
    "\n",
    "result = (T_1 * c.unsqueeze(-1)) + (T_2 * (~c).unsqueeze(-1))\n",
    "\n",
    "# Record end event\n",
    "end_event.record()\n",
    "\n",
    "# Wait for computations to finish\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = start_event.elapsed_time(end_event) / 1000  # Convert to seconds\n",
    "print(\"Time taken:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4, 5)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "def custom_operation(T_1, T_2, c_star):\n",
    "    # Element-wise multiplication with c_star and its complement\n",
    "    term1 = T_1 * c_star[..., None]\n",
    "    term2 = T_2 * (~c_star)[..., None]\n",
    "    \n",
    "    # Sum the terms along the joint dimension\n",
    "    T = term1 + term2\n",
    "    \n",
    "    return T\n",
    "\n",
    "# Example usage\n",
    "batch_size = 3\n",
    "num_legs = 4\n",
    "num_of_joints_per_leg = 5\n",
    "\n",
    "# Random tensors for T_1, T_2, and c_star\n",
    "T_1 = jax.random.normal(jax.random.PRNGKey(0), (batch_size, num_legs, num_of_joints_per_leg))\n",
    "T_2 = jax.random.normal(jax.random.PRNGKey(1), (batch_size, num_legs, num_of_joints_per_leg))\n",
    "c_star = jax.random.randint(jax.random.PRNGKey(2), (batch_size, num_legs), 0, 2)\n",
    "\n",
    "# Perform custom operation\n",
    "T = custom_operation(T_1, T_2, c_star)\n",
    "\n",
    "print(T.shape)  # Output shape should be (batch_size, num_legs, num_of_joints_per_leg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4, 5)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "\n",
    "@jit\n",
    "def custom_operation(T_1, T_2, c_star):\n",
    "    # Element-wise multiplication with c_star and its complement\n",
    "    term1 = T_1 * c_star[..., None]\n",
    "    term2 = T_2 * (~c_star)[..., None]\n",
    "    \n",
    "    # Sum the terms along the joint dimension\n",
    "    T = term1 + term2\n",
    "    \n",
    "    return T\n",
    "\n",
    "# Example usage\n",
    "batch_size = 3\n",
    "num_legs = 4\n",
    "num_of_joints_per_leg = 5\n",
    "\n",
    "# Random tensors for T_1, T_2, and c_star\n",
    "T_1 = jax.random.normal(jax.random.PRNGKey(0), (batch_size, num_legs, num_of_joints_per_leg))\n",
    "T_2 = jax.random.normal(jax.random.PRNGKey(1), (batch_size, num_legs, num_of_joints_per_leg))\n",
    "c_star = jax.random.randint(jax.random.PRNGKey(2), (batch_size, num_legs), 0, 2)\n",
    "\n",
    "# Move tensors to GPU\n",
    "T_1 = jax.device_put(T_1, jax.devices('gpu')[0])\n",
    "T_2 = jax.device_put(T_2, jax.devices('gpu')[0])\n",
    "c_star = jax.device_put(c_star, jax.devices('gpu')[0])\n",
    "\n",
    "# Perform custom operation\n",
    "T = custom_operation(T_1, T_2, c_star)\n",
    "\n",
    "print(T.shape)  # Output shape should be (batch_size, num_legs, num_of_joints_per_leg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.00010966400057077407 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create some tensors for demonstration\n",
    "batch_size = 4096\n",
    "num_legs = 4\n",
    "num_of_joints_per_leg = 3\n",
    "T_1 = jax.random.normal(jax.random.PRNGKey(0), (batch_size, num_legs, num_of_joints_per_leg))\n",
    "T_2 = jax.random.normal(jax.random.PRNGKey(1), (batch_size, num_legs, num_of_joints_per_leg))\n",
    "c_star = jax.random.randint(jax.random.PRNGKey(2), (batch_size, num_legs), 0, 2)\n",
    "\n",
    "# Create CUDA events\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "# Record start event\n",
    "start_event.record()\n",
    "\n",
    "# Example operation (e.g., matrix multiplication)\n",
    "T = custom_operation(T_1, T_2, c_star)\n",
    "\n",
    "# result = (T_1 * c.unsqueeze(-1)) + (T_2 * (~c).unsqueeze(-1))\n",
    "\n",
    "# Record end event\n",
    "end_event.record()\n",
    "\n",
    "# Wait for computations to finish\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = start_event.elapsed_time(end_event) / 1000  # Convert to seconds\n",
    "print(\"Time taken:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "num_legs = 4\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n",
      "[4, 5, 6, 7]\n",
      "[8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "print(a[:num_legs])\n",
    "print(a[num_legs:2*num_legs])\n",
    "print(a[2*num_legs:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gait Generator\n",
    "from : f, d, phase, time_horizon  \n",
    "return : c, new_phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Args:\n",
    "            - f   (torch.Tensor): Leg frequency                         of shape(batch_size, num_legs, parallel_rollout)\n",
    "            - d   (torch.Tensor): Stepping duty cycle                   of shape(batch_size, num_legs, parallel_rollout)\n",
    "            - phase (tch.Tensor): phase of leg                          of shape(batch_size, num_legs, parallel_rollout)\n",
    "            - time_horizon (int): Time horizon for the contact sequence\n",
    "\n",
    "        Returns:\n",
    "            - c     (torch.bool): Foot contact sequence                 of shape(batch_size, num_legs, time_horizon, parallel_rollout)\n",
    "            - phase (tch.Tensor): The phase updated by one time steps   of shape(batch_size, num_legs, parallel_rollout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Leg Frequency : f ----\n",
      "f shape: torch.Size([1, 2, 3])\n",
      "f : tensor([[[1., 2., 3.],\n",
      "         [1., 2., 3.]]])\n",
      "\n",
      "---- Stepping duty cycle : d ----\n",
      "d shape: torch.Size([1, 2, 3])\n",
      "d : tensor([[[2., 2., 2.],\n",
      "         [2., 2., 2.]]])\n",
      "\n",
      "---- Phase ----\n",
      "phase shape: torch.Size([1, 2, 3])\n",
      "phase : tensor([[[1., 2., 3.],\n",
      "         [1., 2., 3.]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_envs = 1\n",
    "_num_legs = 2\n",
    "parallel_rollout = 3\n",
    "device = 'cpu'\n",
    "\n",
    "time_horizon = 4\n",
    "dt = 0.1\n",
    "\n",
    "f = torch.zeros(num_envs, _num_legs, parallel_rollout, device=device)\n",
    "f = torch.Tensor([1,2,3]).expand(num_envs,_num_legs, parallel_rollout)\n",
    "print('---- Leg Frequency : f ----')\n",
    "print('f shape:', f.shape)\n",
    "print('f :', f)\n",
    "print()\n",
    "\n",
    "d = torch.zeros(num_envs, _num_legs, parallel_rollout, device=device)\n",
    "d = d+2\n",
    "print('---- Stepping duty cycle : d ----')\n",
    "print('d shape:', d.shape)\n",
    "print('d :', d)\n",
    "print()\n",
    "\n",
    "phase = torch.zeros(num_envs, _num_legs, parallel_rollout, device=device)\n",
    "phase = torch.Tensor([1,2,3]).expand(num_envs,_num_legs, parallel_rollout)\n",
    "print('---- Phase ----')\n",
    "print('phase shape:', phase.shape)\n",
    "print('phase :', phase)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.2000, 0.3000, 0.4000])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(start=1, end=time_horizon, steps=time_horizon)*dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3])\n",
      "torch.Size([1, 2, 3, 1])\n",
      "torch.Size([1, 2, 3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1., 1.],\n",
       "          [2., 2., 2., 2.],\n",
       "          [3., 3., 3., 3.]],\n",
       "\n",
       "         [[1., 1., 1., 1.],\n",
       "          [2., 2., 2., 2.],\n",
       "          [3., 3., 3., 3.]]]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(phase.shape)\n",
    "print(phase.unsqueeze(-1).shape)\n",
    "print(phase.unsqueeze(-1).expand(*[-1] * len(phase.shape),time_horizon).shape)\n",
    "phase.unsqueeze(-1).expand(num_envs,_num_legs,parallel_rollout,time_horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.1000, 1.2000, 1.3000, 1.4000],\n",
       "          [2.2000, 2.4000, 2.6000, 2.8000],\n",
       "          [3.3000, 3.6000, 3.9000, 4.2000]],\n",
       "\n",
       "         [[1.1000, 1.2000, 1.3000, 1.4000],\n",
       "          [2.2000, 2.4000, 2.6000, 2.8000],\n",
       "          [3.3000, 3.6000, 3.9000, 4.2000]]]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_phases = phase.unsqueeze(-1).expand(num_envs,_num_legs,parallel_rollout,time_horizon) + f.unsqueeze(-1).expand(num_envs,_num_legs,parallel_rollout,time_horizon)*torch.linspace(start=1, end=time_horizon, steps=time_horizon)*dt\n",
    "\n",
    "print(new_phases.shape)\n",
    "new_phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1000, 0.2000, 0.3000, 0.4000],\n",
       "          [0.2000, 0.4000, 0.6000, 0.8000],\n",
       "          [0.3000, 0.6000, 0.9000, 0.2000]],\n",
       "\n",
       "         [[0.1000, 0.2000, 0.3000, 0.4000],\n",
       "          [0.2000, 0.4000, 0.6000, 0.8000],\n",
       "          [0.3000, 0.6000, 0.9000, 0.2000]]]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_phases = new_phases%1\n",
    "\n",
    "print(new_phases.shape)\n",
    "new_phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1000, 0.2000, 0.3000],\n",
       "         [0.1000, 0.2000, 0.3000]]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_phase = new_phases[...,0]\n",
    "\n",
    "print(new_phase.shape)\n",
    "new_phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = new_phases > d.unsqueeze(-1).expand(*[-1] * len(d.shape),time_horizon)\n",
    "\n",
    "print(c.shape)\n",
    "c.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gait_generator(f, d, phase, time_horizon):\n",
    "    new_phases = phase.unsqueeze(-1).expand(*[-1] * len(phase.shape),time_horizon) + f.unsqueeze(-1).expand(*[-1] * len(f.shape),time_horizon)*torch.linspace(start=1, end=time_horizon, steps=time_horizon)*dt\n",
    "\n",
    "    new_phases = new_phases%1\n",
    "\n",
    "    new_phase = new_phases[..., 0]\n",
    "\n",
    "    c = new_phases > d.unsqueeze(-1).expand(*[-1] * len(d.shape),time_horizon)\n",
    "\n",
    "    return c, new_phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor shape: torch.Size([1, 2, 1, 3, 1])\n",
      "Tensor shape after squeezing: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor (size can be unknown)\n",
    "tensor = torch.randn(1, 2, 1, 3, 1)\n",
    "\n",
    "# Squeeze out singleton dimensions\n",
    "unsqueezed_tensor = tensor.squeeze()\n",
    "\n",
    "print(\"Original tensor shape:\", tensor.shape)\n",
    "print(\"Tensor shape after squeezing:\", unsqueezed_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saved Hsitory of Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # # parse the body index\n",
    "        # body_ids, body_names = self._asset.find_bodies(self.cfg.body_name)\n",
    "        # if len(body_ids) != 1:\n",
    "        #     raise ValueError(\n",
    "        #         f\"Expected one match for the body name: {self.cfg.body_name}. Found {len(body_ids)}: {body_names}.\"\n",
    "        #     )\n",
    "        # # save only the first body index\n",
    "        # self._body_idx = body_ids[0]\n",
    "        # self._body_name = body_names[0]\n",
    "        # # check if articulation is fixed-base\n",
    "        # # if fixed-base then the jacobian for the base is not computed\n",
    "        # # this means that number of bodies is one less than the articulation's number of bodies\n",
    "        # if self._asset.is_fixed_base:\n",
    "        #     self._jacobi_body_idx = self._body_idx - 1\n",
    "        # else:\n",
    "        #     self._jacobi_body_idx = self._body_idx\n",
    "        # carb.log_info(  # log info for debugging\n",
    "        #     f\"Resolved body name for the action term {self.__class__.__name__}: {self._body_name} [{self._body_idx}]\"\n",
    "        # )\n",
    "\n",
    "        # # convert the fixed offsets to torch tensors of batched shape\n",
    "        # if self.cfg.body_offset is not None:\n",
    "        #     self._offset_pos = torch.tensor(self.cfg.body_offset.pos, device=self.device).repeat(self.num_envs, 1)\n",
    "        #     self._offset_rot = torch.tensor(self.cfg.body_offset.rot, device=self.device).repeat(self.num_envs, 1)\n",
    "        # else:\n",
    "        #     self._offset_pos, self._offset_rot = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # Stolen from DifferentialInverseKinematicsAction\n",
    "    def _compute_frame_jacobian(self):\n",
    "        \"\"\"Computes the geometric Jacobian of the target frame in the root frame.\n",
    "\n",
    "        This function accounts for the target frame offset and applies the necessary transformations to obtain\n",
    "        the right Jacobian from the parent body Jacobian.\n",
    "        \"\"\"\n",
    "        # read the parent jacobian\n",
    "        jacobian = self._asset.root_physx_view.get_jacobians()[:, self._jacobi_body_idx, :, self._joint_ids]\n",
    "\n",
    "        jacobian = self._asset.root_physx_view.get_jacobians()\n",
    "\n",
    "        \"\"\"Ordered names of bodies in articulation (through rigid body view).\"\"\"\n",
    "        prim_paths = self._asset.body_physx_view.prim_paths[: self._asset.num_bodies]\n",
    "        body_names = [path.split(\"/\")[-1] for path in prim_paths]\n",
    "        print(\"Link names through body view: \", body_names) #['base', 'FL_hip', 'FL_thigh', 'FL_calf', 'FL_foot', 'FR_hip', 'FR_thigh', 'FR_calf', 'FR_foot', 'RL_hip', 'RL_thigh', 'RL_calf', 'RL_foot', 'RR_hip', 'RR_thigh', 'RR_calf', 'RR_foot']\n",
    "\n",
    "        \"\"\"Ordered names of bodies in articulation (through articulation view).\"\"\"\n",
    "        body_names = self._asset.root_physx_view.shared_metatype.link_names\n",
    "        print(\"Link names through articulation view: \", body_names) #['base', 'FL_hip', 'FR_hip', 'RL_hip', 'RR_hip', 'FL_thigh', 'FR_thigh', 'RL_thigh', 'RR_thigh', 'FL_calf', 'FR_calf', 'RL_calf', 'RR_calf', 'FL_foot', 'FR_foot', 'RL_foot', 'RR_foot']\n",
    "\n",
    "        # account for the offset\n",
    "        if self.cfg.body_offset is not None:\n",
    "            # Modify the jacobian to account for the offset\n",
    "            # -- translational part\n",
    "            # v_link = v_ee + w_ee x r_link_ee = v_J_ee * q + w_J_ee * q x r_link_ee\n",
    "            #        = (v_J_ee + w_J_ee x r_link_ee ) * q\n",
    "            #        = (v_J_ee - r_link_ee_[x] @ w_J_ee) * q\n",
    "            jacobian[:, 0:3, :] += torch.bmm(-math_utils.skew_symmetric_matrix(self._offset_pos), jacobian[:, 3:, :])\n",
    "            # -- rotational part\n",
    "            # w_link = R_link_ee @ w_ee\n",
    "            jacobian[:, 3:, :] = torch.bmm(math_utils.matrix_from_quat(self._offset_rot), jacobian[:, 3:, :])\n",
    "\n",
    "        return jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def apply_actions(self):\n",
    "        \"\"\"Applies the actions to the asset managed by the term.\n",
    "        Note: This is called at every simulation step by the manager.\n",
    "        \"\"\"\n",
    "        output_torques = (torch.rand(self.num_envs, self._num_joints, device=self.device))# * 80) - 40\n",
    "\n",
    "        # print('--- Torch ---')\n",
    "        # print('shape : ',output_torques.shape)\n",
    "        # print('device : ',output_torques.device)\n",
    "        # print('Type : ', type(output_torques))\n",
    "        \n",
    "        output_torques_jax = torch_to_jax(output_torques)\n",
    "        output_torques_jax = (output_torques_jax * 80) - 40\n",
    "\n",
    "        # print('')\n",
    "        # print('--- Jax ---')\n",
    "        # print('Shape : ', output_torques_jax.shape)\n",
    "        # print('device : ',output_torques_jax.devices())\n",
    "        # print('Type : ', type(output_torques_jax))\n",
    "\n",
    "        output_torques2 = jax_to_torch(output_torques_jax)\n",
    "\n",
    "        # set joint effort targets (should be equivalent to torque) : Torque controlled robot\n",
    "        self._asset.set_joint_effort_target(output_torques2, joint_ids=self._joint_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_robot_state2(self):\n",
    "        \"\"\" TODO Write description\n",
    "        \"\"\"\n",
    "\n",
    "        # Joint Index\n",
    "        fl_joints = self._asset.find_joints(\"FL.*\")[0]\t\t# list [0, 4,  8]\n",
    "        fr_joints = self._asset.find_joints(\"FR.*\")[0]\t\t# list [1, 5,  9]\n",
    "        rl_joints = self._asset.find_joints(\"RL.*\")[0]\t\t# list [2, 6, 10]\n",
    "        rr_joints = self._asset.find_joints(\"RR.*\")[0]\t\t# list [3, 7, 11]\n",
    "\n",
    "        # Body Index\n",
    "        foot_idx = self._asset.find_bodies(\".*foot\")[0]\n",
    "\n",
    "        # 'FL_foot', 'FR_foot', 'RL_foot', 'RR_foot'\n",
    "        fl_jacobian = self._asset.root_physx_view.get_jacobians()[:, foot_idx[0], 0:3, fl_joints]# + 6]\n",
    "        fr_jacobian = self._asset.root_physx_view.get_jacobians()[:, foot_idx[1], 0:3, fr_joints]# + 6]\n",
    "        rl_jacobian = self._asset.root_physx_view.get_jacobians()[:, foot_idx[2], 0:3, rl_joints]# + 6]\n",
    "        rr_jacobian = self._asset.root_physx_view.get_jacobians()[:, foot_idx[3], 0:3, rr_joints]# + 6]\n",
    "\n",
    "        # foot position in wf\n",
    "        fl_foot_pos_w = self._asset.data.body_state_w[:, foot_idx[0], 0:3]\n",
    "        fr_foot_pos_w = self._asset.data.body_state_w[:, foot_idx[1], 0:3]\n",
    "        rl_foot_pos_w = self._asset.data.body_state_w[:, foot_idx[2], 0:3]\n",
    "        rr_foot_pos_w = self._asset.data.body_state_w[:, foot_idx[3], 0:3]\n",
    "\n",
    "        # foot orientation in wf\n",
    "        fl_foot_orient_w = self._asset.data.body_state_w[:, foot_idx[0], 3:7]\n",
    "        fr_foot_orient_w = self._asset.data.body_state_w[:, foot_idx[1], 3:7]\n",
    "        rl_foot_orient_w = self._asset.data.body_state_w[:, foot_idx[2], 3:7]\n",
    "        rr_foot_orient_w = self._asset.data.body_state_w[:, foot_idx[3], 3:7]\n",
    "\n",
    "        # Root state ``[pos, quat, lin_vel, ang_vel]`` in simulation world frame. Shape is (num_instances, 13)\n",
    "        base_pose_w = self._asset.data.root_state_w[:, 0:3]\n",
    "        base_orient_w = self._asset.data.root_state_w[:, 3:7]\n",
    "        base_lin_vel_w = self._asset.data.root_state_w[:, 7:10]\n",
    "        base_ang_vel_w = self._asset.data.root_state_w[:, 10:13]\n",
    "\n",
    "        # foot position, orientation in bf\n",
    "        fl_foot_pos_b, fl_foot_orient_b = math_utils.subtract_frame_transforms(base_pose_w, base_orient_w, fl_foot_pos_w, fl_foot_orient_w)\n",
    "        fr_foot_pos_b, fr_foot_orient_b = math_utils.subtract_frame_transforms(base_pose_w, base_orient_w, fr_foot_pos_w, fr_foot_orient_w)\n",
    "        rl_foot_pos_b, rl_foot_orient_b = math_utils.subtract_frame_transforms(base_pose_w, base_orient_w, rl_foot_pos_w, rl_foot_orient_w)\n",
    "        rr_foot_pos_b, rr_foot_orient_b = math_utils.subtract_frame_transforms(base_pose_w, base_orient_w, rr_foot_pos_w, rr_foot_orient_w)\n",
    "\n",
    "        # foot joint position\n",
    "        fl_joint_pos = self._asset.data.joint_pos[:, fl_joints]\n",
    "        fr_joint_pos = self._asset.data.joint_pos[:, fr_joints]\n",
    "        rl_joint_pos = self._asset.data.joint_pos[:, rl_joints]\n",
    "        rr_joint_pos = self._asset.data.joint_pos[:, rr_joints]\n",
    "\n",
    "        # foot joint velocity\n",
    "        fl_joint_vel = self._asset.data.joint_vel[:, fl_joints]\n",
    "        fr_joint_vel = self._asset.data.joint_vel[:, fr_joints]\n",
    "        rl_joint_vel = self._asset.data.joint_vel[:, rl_joints]\n",
    "        rr_joint_vel = self._asset.data.joint_vel[:, rr_joints]\n",
    "\n",
    "        print('alo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stance Leg controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor Shape: torch.Size([2, 4, 3, 5])\n",
      "Random Tensor:\n",
      "tensor([[[[-1.0008,  0.7120, -0.1742, -1.4273, -0.2322],\n",
      "          [ 0.4963, -0.0760,  0.7140,  0.5691, -0.4790],\n",
      "          [-0.2406,  0.7622,  0.0441, -2.3687,  0.6843]],\n",
      "\n",
      "         [[-1.1865, -0.2030, -0.8373,  1.5632, -0.9582],\n",
      "          [-1.6118,  0.9994,  0.2950,  1.7622, -0.6367],\n",
      "          [-0.9983,  0.1925,  1.1585, -0.3202, -0.7383]],\n",
      "\n",
      "         [[ 0.9787,  0.0971,  0.9300, -1.4056,  0.0978],\n",
      "          [-0.8115,  0.1189,  1.5842,  0.1966,  1.0970],\n",
      "          [-0.2550, -2.1972, -0.2008, -0.6315,  0.5687]],\n",
      "\n",
      "         [[ 0.8719, -0.0730, -0.2174,  0.8004, -0.7905],\n",
      "          [ 0.4587, -0.1622,  0.8989,  1.1916,  0.8667],\n",
      "          [ 0.1111, -1.7449,  0.0858,  1.7471,  0.4848]]],\n",
      "\n",
      "\n",
      "        [[[-2.6416,  1.3883,  1.4118, -0.1556, -0.0260],\n",
      "          [ 0.0520, -1.5354, -0.1618, -0.0765,  0.7076],\n",
      "          [ 0.6039, -0.3753, -0.7691,  0.4189, -1.3773]],\n",
      "\n",
      "         [[-0.7048,  1.1861, -0.2367, -0.0804,  0.8997],\n",
      "          [ 1.2829, -0.1929, -0.9965, -0.4421,  0.7688],\n",
      "          [-1.1038, -0.4555, -0.1341,  0.7287,  0.5386]],\n",
      "\n",
      "         [[ 2.1951, -0.3242, -0.6417,  0.2766, -0.3449],\n",
      "          [ 1.1949,  0.4038,  0.8481, -0.8909,  1.4892],\n",
      "          [ 2.1680,  0.3773, -1.4098,  0.2815, -1.8475]],\n",
      "\n",
      "         [[-0.1910, -0.9337, -0.8457,  2.0190,  0.9930],\n",
      "          [-0.3845, -0.0142,  1.1648,  0.6924,  0.3329],\n",
      "          [ 0.6188,  0.0251, -0.0031, -0.2717,  1.0208]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define symbolic variables\n",
    "batch_size = 2\n",
    "num_legs = 4\n",
    "num_joints_per_leg = 5\n",
    "\n",
    "# Instantiate the tensor with symbolic shape\n",
    "shape = (batch_size, num_legs, 3, num_joints_per_leg)\n",
    "tensor = torch.randn(*shape)\n",
    "\n",
    "print(\"Random Tensor Shape:\", tensor.shape)\n",
    "print(\"Random Tensor:\")\n",
    "print(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean Tensor Shape: torch.Size([2, 4])\n",
      "Boolean Tensor:\n",
      "tensor([[ True,  True, False, False],\n",
      "        [False,  True, False,  True]])\n"
     ]
    }
   ],
   "source": [
    "c0 = torch.randint(0, 2, (batch_size, num_legs), dtype=torch.bool)\n",
    "\n",
    "print(\"Boolean Tensor Shape:\", c0.shape)\n",
    "print(\"Boolean Tensor:\")\n",
    "print(c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobian = torch.randn(batch_size, num_legs, 3, num_joints_per_leg)\n",
    "F0_star = torch.randn(batch_size, num_legs, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian   shape : torch.Size([2, 4, 3, 5])\n",
      "Jacobian.T shape : torch.Size([2, 4, 5, 3])\n",
      "   GRF     shape : torch.Size([2, 4, 3])\n",
      "   GRF 2   shape : torch.Size([2, 4, 3, 1])\n",
      "     q     shape : torch.Size([2, 4, 5, 1])\n",
      "     q2    shape : torch.Size([2, 4, 5])\n",
      "\n",
      "Jacobian : \n",
      "tensor([[ 1.8449, -0.1331, -0.4473, -0.3967, -0.5775],\n",
      "        [-0.1599, -0.0411,  0.1473, -0.2492,  1.2638],\n",
      "        [-1.4937, -1.0205, -0.9950,  0.1527,  1.4755]])\n",
      "\n",
      "GRF : \n",
      "tensor([ 0.8727, -0.0659, -0.1853])\n",
      "\n",
      "Joints : \n",
      "tensor([-1.1362, -1.4506, -0.6089,  2.4870,  1.3955])\n",
      "\n",
      "Torques : \n",
      "tensor([[[-0.0000,  0.0000,  0.0000, -0.0000, -0.0000],\n",
      "         [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "         [-1.8240,  0.8967, -0.6014,  0.1919,  0.6543],\n",
      "         [-0.0195, -0.1334, -1.3107,  0.5556, -2.4532]],\n",
      "\n",
      "        [[ 0.4518,  1.8152,  1.0578, -2.1407,  0.1980],\n",
      "         [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "         [-1.1362, -1.4506, -0.6089,  2.4870,  1.3955],\n",
      "         [ 0.0000,  0.0000, -0.0000, -0.0000, -0.0000]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jacobian = torch.randn(batch_size, num_legs, 3, num_joints_per_leg)\n",
    "jacobian_T = jacobian.transpose(-1,-2).detach().clone() # Transpose last two dimensions\n",
    "print('Jacobian   shape :',jacobian.shape)\n",
    "print('Jacobian.T shape :',jacobian_T.shape)\n",
    "\n",
    "# F0_star = torch.randn(batch_size, num_legs, 3)\n",
    "F0_star2 = F0_star.unsqueeze(-1).detach().clone() \n",
    "print('   GRF     shape :',F0_star.shape)\n",
    "print('   GRF 2   shape :',F0_star2.shape)\n",
    "\n",
    "q = torch.matmul(jacobian_T, F0_star.unsqueeze(-1))\n",
    "q2 = q.squeeze(-1).clone().detach()\n",
    "print('     q     shape :',q.shape)\n",
    "print('     q2    shape :',q2.shape)\n",
    "\n",
    "print('')\n",
    "print('Jacobian : ')\n",
    "print(jacobian[1,3,:,:])\n",
    "\n",
    "print('')\n",
    "print('GRF : ')\n",
    "print(F0_star[1,3,:])\n",
    "\n",
    "print('')\n",
    "print('Joints : ')\n",
    "print(q2[1,2,:])\n",
    "\n",
    "T = q2 * ~c0.unsqueeze(-1).expand(*[-1] * len(c0.shape), T.shape[-1])\n",
    "print('')\n",
    "print('Torques : ')\n",
    "print(T[:,:,:])\n",
    "\n",
    "T.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 5])\n",
      "tensor([[[-0.0000,  0.0000,  0.0000, -0.0000, -0.0000],\n",
      "         [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "         [-1.8240,  0.8967, -0.6014,  0.1919,  0.6543],\n",
      "         [-0.0195, -0.1334, -1.3107,  0.5556, -2.4532]],\n",
      "\n",
      "        [[ 0.4518,  1.8152,  1.0578, -2.1407,  0.1980],\n",
      "         [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "         [-1.1362, -1.4506, -0.6089,  2.4870,  1.3955],\n",
      "         [ 0.0000,  0.0000, -0.0000, -0.0000, -0.0000]]])\n",
      "tensor([[-0.0000,  0.0000,  0.6543, -2.4532],\n",
      "        [ 0.1980, -0.0000,  1.3955, -0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(T.shape)\n",
    "print(T)\n",
    "print(T[:,:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian     Shape: torch.Size([2, 4, 3, 5])\n",
      "Jacobian dot Shape: torch.Size([2, 4, 3, 5])\n",
      "     q   dot Shape: torch.Size([2, 4, 5])\n",
      "Mass Matrix  Shape: torch.Size([2, 4, 5, 5])\n",
      "     h       Shape: torch.Size([2, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define symbolic variables\n",
    "batch_size = 2\n",
    "num_legs = 4\n",
    "num_joints_per_leg = 5\n",
    "\n",
    "jacobian = torch.randn(batch_size, num_legs, 3, num_joints_per_leg)\n",
    "jacobian_dot = torch.randn(batch_size, num_legs, 3, num_joints_per_leg)\n",
    "q_dot = torch.randn(batch_size, num_legs, num_joints_per_leg)\n",
    "mass_matrix = torch.randn(batch_size, num_legs, num_joints_per_leg, num_joints_per_leg)\n",
    "h = torch.randn(batch_size, num_legs, num_joints_per_leg)\n",
    "\n",
    "print(\"Jacobian     Shape:\", jacobian.shape)\n",
    "print(\"Jacobian dot Shape:\", jacobian_dot.shape)\n",
    "print(\"     q   dot Shape:\", q_dot.shape)\n",
    "print(\"Mass Matrix  Shape:\", mass_matrix.shape)\n",
    "print(\"     h       Shape:\", h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   J_dot_x_q_dot Shape: torch.Size([2, 4, 3])\n",
      "                   jacobian_inv  Shape: torch.Size([2, 4, 5, 3])\n",
      "     J[p_dot_dot - J(q)*q_dot] Shape: torch.Size([2, 4, 5])\n",
      "M(q)*J[p_dot_dot - J(q)*q_dot] Shape: torch.Size([2, 4, 5])\n",
      "                              T  Shape: torch.Size([2, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "J_dot_x_q_dot = torch.matmul(jacobian_dot, q_dot.unsqueeze(-1)).squeeze(-1)\n",
    "print(\"                   J_dot_x_q_dot Shape:\", J_dot_x_q_dot.shape)\n",
    "\n",
    "jacobian_inv = torch.linalg.pinv(jacobian)\n",
    "print(\"                   jacobian_inv  Shape:\", jacobian_inv.shape)\n",
    "\n",
    "J_inv_p_dot_dot_min_J_dot_x_q_dot = torch.matmul(jacobian_inv, J_dot_x_q_dot.unsqueeze(-1)).squeeze(-1)\n",
    "print(\"     J[p_dot_dot - J(q)*q_dot] Shape:\", J_inv_p_dot_dot_min_J_dot_x_q_dot.shape)\n",
    "\n",
    "M_J_inv_p_dot_dot_min_J_dot_x_q_dot = torch.matmul(mass_matrix, J_inv_p_dot_dot_min_J_dot_x_q_dot.unsqueeze(-1)).squeeze(-1)\n",
    "print('M(q)*J[p_dot_dot - J(q)*q_dot] Shape:', M_J_inv_p_dot_dot_min_J_dot_x_q_dot.shape)\n",
    "\n",
    "# Final step\n",
    "T = torch.add(M_J_inv_p_dot_dot_min_J_dot_x_q_dot, h)\n",
    "print('                              T  Shape:', T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 17, 6, 18])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define symbolic variables\n",
    "batch_size = 5\n",
    "num_legs = 4\n",
    "num_joints_per_leg = 3\n",
    "\n",
    "_foot_idx = [13,14,15,16]\n",
    "_joint_idx = [[0,4,8],[1,5,9],[2,6,10],[3,7,11]]\n",
    "# _joint_idx = [[[[0,4,8]],[[1,5,9]],[[2,6,10]],[[3,7,11]]]]\n",
    "_joint_idx_tensor = torch.Tensor(_joint_idx)\n",
    "\n",
    "jacobian = torch.randn(batch_size, 17, 6, 18)\n",
    "print(jacobian.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4, 3, 18])\n",
      "0\n",
      "[0, 4, 8]\n",
      "1\n",
      "[1, 5, 9]\n",
      "2\n",
      "[2, 6, 10]\n",
      "3\n",
      "[3, 7, 11]\n"
     ]
    }
   ],
   "source": [
    "print(jacobian[:,_foot_idx,:3,:].shape)\n",
    "\n",
    "for leg_i, joints_in_leg_i in enumerate(_joint_idx):\n",
    "    print(leg_i)\n",
    "    print(joints_in_leg_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 3, 3])\n",
      "torch.Size([5, 3, 3, 3])\n",
      "torch.Size([5, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "jacob1 = jacobian[:, 0, :3, [0,4,8]].unsqueeze(1)\n",
    "jacob2 = jacobian[:, 1, :3, [1,5,9]].unsqueeze(1)\n",
    "jacob3 = jacobian[:, 2, :3, [2,6,10]].unsqueeze(1)\n",
    "print(jacob1.shape)\n",
    "print(torch.cat((jacob1, jacob2, jacob3), dim=1).shape)\n",
    "\n",
    "jacob = []\n",
    "jacob.append(jacobian[:, 0, :3, 6+np.asarray([0,4,8])].unsqueeze(1))\n",
    "jacob.append(jacobian[:, 1, :3, [1,5,9]].unsqueeze(1))\n",
    "jacob.append(jacobian[:, 2, :3, [2,6,10]].unsqueeze(1))\n",
    "print(torch.cat((jacob), dim=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12//3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.0000, 2.0000, 2.1000],\n",
      "         [3.0000, 4.0000, 4.1000]],\n",
      "\n",
      "        [[5.0000, 6.0000, 6.1000],\n",
      "         [7.0000, 8.0000, 8.1000]]])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 3.0000, 2.0000, 4.0000, 2.1000, 4.1000],\n",
       "        [5.0000, 7.0000, 6.0000, 8.0000, 6.1000, 8.1000]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[[1.0, 2.0, 2.1], [3.0, 4.0, 4.1]], [[5,6, 6.1] , [7, 8, 8.1]]])\n",
    "print(a)\n",
    "print()\n",
    "a.permute(0,2,1).reshape(2,6)\n",
    "# a.view(2,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Touch Down Pos : Convolutions\n",
    "\n",
    "        # Utiliser une convolution sur c (contact sequence) pour trouver le point de dpart et d'arriver du pied.\n",
    "        # Avec un filtre genre f = [0, 1], pour ne garder que les flancs montants\n",
    "        # Imaginons p = [p1, p2, p3, p4, p5, p6, p7, p8, p9, p10]\n",
    "        #           c = [ 1,  1,  0,  0,  0,  1,  1,  0,  0,   1]\n",
    "        # Les points de dpart serait p1, p6 et p10 les points d'arriv p6 et p10\n",
    "        # Il faudrait retourner qqch comme \n",
    "        #        key =  [1,   0,  0,  0,  0,  1,  0,  0,  0,   1]\n",
    "        # Qui permetrait d'extraire facilement [p1, p6, p10] avec p[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 1., 0., 0., 0., 1., 1.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 1., 0., 1., 0., 1., 0.],\n",
       "         [0., 0., 0., 1., 0., 1., 0., 0., 1., 0.]]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 2\n",
    "num_legs = 4\n",
    "time_horizon = 10\n",
    "\n",
    "c = torch.empty(batch_size, num_legs, time_horizon).bernoulli(0.2)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000, -0.0000, -0.0000, -0.0000,  0.2320, -0.0000, -0.0000,\n",
       "           0.0000,  0.2659, -1.4655],\n",
       "         [ 0.0000, -0.0000,  0.6376,  0.0000,  0.0000, -0.0000,  0.0000,\n",
       "           0.0000,  1.6942,  0.0000],\n",
       "         [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
       "           0.0000,  1.2197,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          -0.0000, -0.3521,  0.0000],\n",
       "         [ 0.6727,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.4122,\n",
       "           0.0000, -0.0000,  0.0000],\n",
       "         [ 1.9076,  0.0000,  0.0000, -0.0000,  3.6689, -0.0000, -0.4448,\n",
       "          -0.0000,  0.4419,  0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000, -0.7543, -0.0000,  0.6289,  0.0000,\n",
       "           0.0000, -1.0148,  0.0000]]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = torch.randn(c.shape)\n",
    "p2 = p * c\n",
    "p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 2, 0, 8],\n",
       "        [8, 0, 0, 3]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indx = torch.argmax((c!=0).float(), dim=-1)\n",
    "indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2320,  0.6376,  1.4360,  1.2197],\n",
       "        [-0.3521,  0.6727,  1.9076, -0.7543]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(p, -1, indx.unsqueeze(-1)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected padding to be a single integer value or a list of 1 values to match the convolution dimensions, but got padding=[0, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m kernel \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      2\u001b[0m kernel \u001b[38;5;241m=\u001b[39m kernel\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected padding to be a single integer value or a list of 1 values to match the convolution dimensions, but got padding=[0, 1]"
     ]
    }
   ],
   "source": [
    "kernel = torch.tensor([-1, 1], dtype=torch.float32)\n",
    "kernel = kernel.unsqueeze(0).unsqueeze(0).expand(4,1,2)\n",
    "torch.conv1d(c, kernel, groups=4, padding=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.,  1.])\n",
      "torch.Size([2])\n",
      "\n",
      "tensor([[[-1.,  1.]],\n",
      "\n",
      "        [[-1.,  1.]],\n",
      "\n",
      "        [[-1.,  1.]],\n",
      "\n",
      "        [[-1.,  1.]]])\n",
      "torch.Size([4, 1, 2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(kernel)\n",
    "print(kernel.shape)\n",
    "print()\n",
    "\n",
    "kernel2 = kernel.unsqueeze(0).unsqueeze(0).expand(4,1,2)\n",
    "print(kernel2)\n",
    "print(kernel2.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[[1,1,1,1],[2,2,2,2],[3,3,3,3]], [[4,4,4,4], [5,5,5,5], [6,6,6,6]]]).transpose(-1,-2)\n",
    "b = torch.tensor([[[1,2,3,4],[2,2,2,2],[3,3,3,3]], [[4,4,4,4], [5,5,5,5], [6,6,6,6]]]).transpose(-1,-2)\n",
    "print(a.shape)\n",
    "\n",
    "c = torch.cat((a,b), dim=1)\n",
    "c.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n",
      "tensor([[0.9700, 0.4700, 0.3033],\n",
      "        [0.2200, 0.1700, 0.1367]])\n",
      "\n",
      "tensor([[0.9000, 0.8000, 0.7000],\n",
      "        [0.6000, 0.5000, 0.4000]])\n",
      "\n",
      "tensor([[0.9000, 0.4000, 0.2333],\n",
      "        [0.1500, 0.1000, 0.0667]])\n",
      "\n",
      "tensor([[0.9700, 0.4700, 0.3033],\n",
      "        [0.2200, 0.1700, 0.1367]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "f = torch.tensor([[1,2,3], [4, 5, 6]])\n",
    "d = torch.tensor([[0.1,0.2,0.3], [0.4, 0.5, 0.6]])\n",
    "print(f.shape)\n",
    "print(d.shape)\n",
    "\n",
    "swing_period = ((1-d) / f) + 0.07\n",
    "print(swing_period.shape)\n",
    "print(swing_period)\n",
    "print()\n",
    "print(1-d)\n",
    "print()\n",
    "print((1-d)/f)\n",
    "print()\n",
    "print((1-d)/f + 0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 1],\n",
       "        [4, 5, 1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[:,-1] = 1\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True, False, False],\n",
      "        [False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "c = torch.empty(2, 3).bernoulli(0.5).bool()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.2000, 0.3000],\n",
       "        [0.4000, 0.5000, 0.6000]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(f * c) + (d * ~c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 2.0000, 0.0500],\n",
       "         [1.5000, 2.0000, 0.0500],\n",
       "         [2.0000, 2.0000, 0.0500],\n",
       "         [2.5000, 2.0000, 0.0500]],\n",
       "\n",
       "        [[4.0000, 5.0000, 0.0500],\n",
       "         [4.0000, 5.0000, 0.0500],\n",
       "         [4.0000, 5.0000, 0.0500],\n",
       "         [4.0000, 5.0000, 0.0500]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(a.shape[:-1])\n",
    "x = (a[:,:,:2] + b[:,:,:2]) / 2\n",
    "torch.cat((x,0.05*torch.ones_like(a[:,:, :1])), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1000, 0.2000, 0.3000],\n",
      "        [0.4000, 0.5000, 0.6000]])\n",
      "\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.0400, 0.0270],\n",
       "        [0.0256, 0.0312, 0.0467]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(d)\n",
    "print()\n",
    "print(f)\n",
    "print()\n",
    "\n",
    "d**f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 3])\n",
      "\n",
      "tensor([[[1, 2, 3],\n",
      "         [1, 2, 3],\n",
      "         [1, 2, 3],\n",
      "         [1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6],\n",
      "         [4, 5, 6],\n",
      "         [4, 5, 6],\n",
      "         [4, 5, 6]]])\n",
      "tensor([[2, 2, 2, 2],\n",
      "        [5, 5, 5, 5]])\n",
      "torch.Size([2, 4])\n",
      "\n",
      "tensor([[2, 2, 2, 2],\n",
      "        [5, 5, 5, 5]])\n",
      "torch.Size([2, 4])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(a.shape)\n",
    "print()\n",
    "print(a)\n",
    "\n",
    "print(a[..., 1])\n",
    "print(a[..., 1].shape)\n",
    "print()\n",
    "\n",
    "print(a[:,:, 1])\n",
    "print(a[:,:, 1].shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "jax.devices()\n",
    "\n",
    "import os\n",
    "print(os.environ.get(\"CUDA_HOME\"))  # Check CUDA_HOME\n",
    "print(os.environ.get(\"LD_LIBRARY_PATH\"))  # Check LD_LIBRARY_PATH (Linux/macOS)\n",
    "print(os.environ.get(\"PATH\"))  # Check PATH (Windows)\n",
    "\n",
    "print('-------------')\n",
    "import torch\n",
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " p0 shape : torch.Size([3, 4, 3])\n",
      " p1 shape : torch.Size([3, 4, 3])\n",
      " p2 shape : torch.Size([3, 4, 3])\n",
      " is_S0 shape : torch.Size([3, 4, 3])\n",
      "\n",
      " cp1 shape : torch.Size([3, 4, 3])\n",
      " cp2 shape : torch.Size([3, 4, 3])\n",
      " cp3 shape : torch.Size([3, 4, 3])\n",
      " cp4 shape : torch.Size([3, 4, 3])\n",
      " time_fac shape : torch.Size([3, 4])\n",
      " t shape : torch.Size([3, 4])\n",
      "\n",
      "time traj shape : torch.Size([3, 4, 5])\n",
      "\n",
      "time traj shape : torch.Size([3, 4, 1, 5])\n",
      " cp1 shape : torch.Size([3, 4, 3, 1])\n",
      " cp2 shape : torch.Size([3, 4, 3, 1])\n",
      " cp3 shape : torch.Size([3, 4, 3, 1])\n",
      " cp4 shape : torch.Size([3, 4, 3, 1])\n",
      "\n",
      "desired_foot_pos_traj shape : torch.Size([3, 4, 3, 5])\n",
      "desired_foot_vel_traj shape : torch.Size([3, 4, 3, 5])\n",
      "desired_foot_acc_traj shape : torch.Size([3, 4, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\"\"\" Given feet position sequence and contact sequence -> compute swing trajectories by fitting a cubic spline between\n",
    "the lift-off and the touch down define in the contact sequence. \n",
    "- Swing frequency and duty cycle are used to compute the swing period\n",
    "- A middle point is used for the interpolation : which is heuristically defined. It defines the step height\n",
    "- p1 (middle point) and p2 (touch-down) are updated each time, while p0 is conserved (always the same lift off position)\n",
    "\n",
    "Args:\n",
    "    - p   (torch.Tensor): Foot position sequence                of shape(batch_size, num_legs, 3, time_horizon)\n",
    "    - c   (torch.Tensor): Foot contact sequence                 of shape(c)\n",
    "    - f   (torch.Tensor): Leg frequency                         of shape(batch_size, num_legs)\n",
    "    - d   (torch.Tensor): Stepping duty cycle                   of shape(batch_size, num_legs)\n",
    "    - decimation   (int): Number of timestep for the traj.\n",
    "\n",
    "Returns:\n",
    "    - pt  (torch.Tensor): Desired Swing Leg trajectories        of shape(batch_size, num_legs, 9, decimation)   (9 = xyz_pos, xzy_vel, xyz_acc)\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 3\n",
    "num_legs = 4\n",
    "time_horizon = 5\n",
    "dt_out = 0.1\n",
    "dt_in = 0.1\n",
    "\n",
    "d = torch.randn(batch_size, num_legs)\n",
    "f = torch.randn(batch_size, num_legs)\n",
    "p = torch.randn(batch_size, num_legs, 3, time_horizon)\n",
    "c = torch.empty(batch_size, num_legs, time_horizon).bernoulli(0.5).bool()\n",
    "c_prev = torch.randn(batch_size, num_legs)\n",
    "p0 = torch.randn(batch_size, num_legs, 3)\n",
    "swing_time = torch.randn(batch_size, num_legs)\n",
    "\n",
    "\n",
    "# Step 0. Define and Compute usefull variables\n",
    "\n",
    "# Heuristic TODO Save that on the right place, could also be a RL variable\n",
    "step_height = 0.05\n",
    "\n",
    "# Time during wich the leg is in swing. TODO Why +0.07 ? Is it an heuristic also ?\n",
    "# Shape (batch_size, num_legs)\n",
    "swing_period = ((1-d) / f) + 0.07\n",
    "\n",
    "half_swing_period = swing_period / 2\n",
    "time_fac = 1 / (swing_period / 2) #bezier_time_factor\n",
    "\n",
    "\n",
    "# Step 1. Retrieve the three interpolation points : p0, p1, p2 (lift-off, middle point, touch down)\n",
    "\n",
    "# Retrieve p0 : If c(0)=0 and c(-1)=1 : The leg lift-off -> p0 = p(0) # TODO p(0) or must it be from simulation data ? TODO Must it be p(0) or p(-1)\n",
    "# Update only the p0 that are new lift off positions (unsqueeze lifting off -> shape(batc_size, num_legs, 1) to make it compatible for multiplication with p shape)\n",
    "# shape (batch_size, num_legs, 3) \n",
    "lifting_off = ((c[:,:,0]==0) * (c_prev == 1)).unsqueeze(-1)\n",
    "p0 = (p[:,:,:,0] * lifting_off) + (p0 * ~lifting_off)  \n",
    "\n",
    "# Retrieve p2 : Retrieve the index of the touch down in the contact sequence : First Non-zero Index : shape(batch_size, num_legs)\n",
    "# Set the last value of c as ONE to avoid the case of only 0 in the contact sequence, wich return the first element (make more sense to retrun the last)\n",
    "# With the touch_down index, retrieve the touch down foot position : p2. !!! Need to chanhe Idx shape from (batch_size, num_legs) to (batch_size, num_legs, 3, 1)!!!\n",
    "# shape (batch_size, num_legs, 3) \n",
    "c[:,:,-1] = 1 # TODO Does it modify c also outside this function ?\n",
    "first_non_zero_indx = torch.argmax((c!=0).float(), dim=-1)\n",
    "\n",
    "# print(' p shape :', p.shape)\n",
    "# print(' Idx shape :', first_non_zero_indx.shape)\n",
    "# print(first_non_zero_indx)\n",
    "# print()\n",
    "# print(' Idx shape :', first_non_zero_indx.unsqueeze(-1).expand(-1,-1,3).shape)\n",
    "# print(first_non_zero_indx.unsqueeze(-1).expand(-1,-1,3))\n",
    "# print()\n",
    "# print()\n",
    "p2 = torch.gather(p, -1, first_non_zero_indx.unsqueeze(-1).unsqueeze(-1)).squeeze(-1)\n",
    "p2 = torch.gather(p, -1, first_non_zero_indx.unsqueeze(-1).expand(-1,-1,3).unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "# Retrieve p1 : (x,y) position are define as the middle point between p0 and p1 (lift-off and touch-down). z is heuristcally define\n",
    "# shape (batch_size, num_legs, 3)\n",
    "p1 = (p0[:,:,:2] + p2[:,:,:2]) / 2     # p1(x,y) is in the middle of p0 and p2\n",
    "p1 = torch.cat((p1, step_height*torch.ones_like(p1[:,:,:1])), dim=2) # Append a third dimension z : defined as step_height\n",
    "\n",
    "# Step 2. Compute the parameters for the interpolation\n",
    "\n",
    "# Swing time : reset if lifting off, then increment by one time step (outer loop) (squeeze lifting_off : (batch_size, num_legs, 1)->(batch_size, num_legs))\n",
    "# then compute t in [0, Delta_t/2], which would be use for the spline interpolation\n",
    "# shape (batch_size, num_legs)\n",
    "# print()\n",
    "# print(' swing_time shape :', swing_time.shape)\n",
    "# print(' lifting_off shape :', lifting_off.shape)\n",
    "swing_time = (swing_time * ~lifting_off.squeeze(-1)) + dt_out\n",
    "t = swing_time % half_swing_period  # Swing time (half)\n",
    "\n",
    "# Compute the a,b,c,d polynimial coefficient for the cubic interpolation S(t) = a*t^3 + b*t^2 + c*t + d\n",
    "# If swing_time < swing period/2 -> S_0(t) (ie. first interpolation), otherwise -> S_1(t - delta_t/2) (ie. second interpolation)\n",
    "# cp_x shape (batch_size, num_legs, 3)\n",
    "print()\n",
    "print(' p0 shape :', p0.shape)\n",
    "print(' p1 shape :', p1.shape)\n",
    "print(' p2 shape :', p2.shape)\n",
    "print(' is_S0 shape :', is_S0.shape)\n",
    "\n",
    "is_S0 = (swing_time <=  half_swing_period).unsqueeze(-1).expand(*[-1] * len(swing_time.shape), 3)  # shape (batch_size, num_legs, 3)\n",
    "cp1 = (p0 * is_S0)                                         + (p1 * ~is_S0)\n",
    "cp2 = (p0 * is_S0)                                         + (torch.cat((p2[:,:,:2], p1[:,:,2:]), dim=2)* ~is_S0)\n",
    "cp3 = (torch.cat((p0[:,:,:2], p1[:,:,2:]), dim=2) * is_S0) + (p2 * ~is_S0)\n",
    "cp4 = (p1 * is_S0)                                              + (p2 * ~is_S0)\n",
    "\n",
    "print()\n",
    "print(' cp1 shape :', cp1.shape)\n",
    "print(' cp2 shape :', cp2.shape)\n",
    "print(' cp3 shape :', cp3.shape)\n",
    "print(' cp4 shape :', cp4.shape)\n",
    "print(' time_fac shape :', time_fac.shape)\n",
    "print(' t shape :', t.shape)\n",
    "\n",
    "\n",
    "# time_fac, t : shape(batch_size, num_legs) -> unsqueezed(-1) -> Shape (batch_size, num_legs, 1)\n",
    "# (arrange = [0,1,2,...])*dt.unsqueeze(0).unsqueeze(-1)       -> Shape (1, 1, time_horizon)\n",
    "# time traj : Shape (batch_size, num_legs, time_horizon)\n",
    "time_traj = (time_fac*t).unsqueeze(-1) + (torch.arange(time_horizon)*dt_in).unsqueeze(0).unsqueeze(0)\n",
    "print()\n",
    "print('time traj shape :', time_traj.shape)\n",
    "\n",
    "cp1 = cp1.unsqueeze(-1)\n",
    "cp2 = cp2.unsqueeze(-1)\n",
    "cp3 = cp3.unsqueeze(-1)\n",
    "cp4 = cp4.unsqueeze(-1)\n",
    "\n",
    "time_traj = time_traj.unsqueeze(2)\n",
    "print()\n",
    "print('time traj shape :', time_traj.shape)\n",
    "print(' cp1 shape :', cp1.shape)\n",
    "print(' cp2 shape :', cp2.shape)\n",
    "print(' cp3 shape :', cp3.shape)\n",
    "print(' cp4 shape :', cp4.shape)\n",
    "\n",
    "\n",
    "# Step 3. Compute the interpolation trajectory\n",
    "desired_foot_pos_traj = cp1*(1 - time_traj)**3 + 3*cp2*(time_traj)*(1 - time_traj)**2 + 3*cp3*((time_traj)**2)*(1 - time_traj) + cp4*(time_traj)**3\n",
    "desired_foot_vel_traj = 3*(cp2 - cp1)*(1 - time_traj)**2 + 6*(cp3 - cp2)*(1 - time_traj)*(time_traj) + 3*(cp4 - cp3)*(time_traj)**2\n",
    "desired_foot_acc_traj = 6*(1 - time_traj) * (cp3 - 2*cp2 + cp1) + 6 * (time_traj) * (cp4 - 2*cp3 + cp2)\n",
    "pt = torch.cat((desired_foot_pos_traj, desired_foot_vel_traj, desired_foot_acc_traj), dim=2)\n",
    "\n",
    "print()\n",
    "print('desired_foot_pos_traj shape :', desired_foot_pos_traj.shape)\n",
    "print('desired_foot_vel_traj shape :', desired_foot_vel_traj.shape)\n",
    "print('desired_foot_acc_traj shape :', desired_foot_acc_traj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swing_trajectory_generator(self, p: torch.Tensor, c: torch.Tensor, f: torch.Tensor, d: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Given feet position sequence and contact sequence -> compute swing trajectories by fitting a cubic spline between\n",
    "        the lift-off and the touch down define in the contact sequence. \n",
    "        - Swing frequency and duty cycle are used to compute the swing period\n",
    "        - A middle point is used for the interpolation : which is heuristically defined. It defines the step height\n",
    "        - p1 (middle point) and p2 (touch-down) are updated each time, while p0 is conserved (always the same lift off position)\n",
    "\n",
    "        Args:\n",
    "            - p   (torch.Tensor): Foot position sequence                of shape(batch_size, num_legs, 3, time_horizon)\n",
    "            - c   (torch.Tensor): Foot contact sequence                 of shape(batch_size, num_legs, time_horizon)\n",
    "            - f   (torch.Tensor): Leg frequency                         of shape(batch_size, num_legs)\n",
    "            - d   (torch.Tensor): Stepping duty cycle                   of shape(batch_size, num_legs)\n",
    "\n",
    "        Returns:\n",
    "            - pt  (torch.Tensor): Desired Swing Leg trajectories        of shape(batch_size, num_legs, 9, decimation)   (9 = xyz_pos, xzy_vel, xyz_acc)\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 0. Define and Compute usefull variables\n",
    "\n",
    "        # Heuristic TODO Save that on the right place, could also be a RL variable\n",
    "        step_height = 0.05\n",
    "\n",
    "        # Time during wich the leg is in swing.(add small numerical value to denominator to avoid division by 0)\n",
    "        # Shape (batch_size, num_legs)\n",
    "        swing_period = ((1-d) / (f.abs()+1e-10))\n",
    "        half_swing_period = swing_period / 2\n",
    "        time_fac = 1 / ((swing_period.abs()+1e-10) / 2) #bezier_time_factor\n",
    "\n",
    "\n",
    "        # Step 1. Retrieve the three interpolation points : p0, p1, p2 (lift-off, middle point, touch down)\n",
    "\n",
    "        # Retrieve p0 : If c(0)=0 and c(-1)=1 : The leg lift-off -> p0 = p(-1) (value from simulation : the last value where c=1)\n",
    "        # Update only the p0 that are new lift off positions (unsqueeze lifting off -> shape(batc_size, num_legs, 1) to make it compatible for multiplication with p shape)\n",
    "        # p0 shape (batch_size, num_legs, 3) \n",
    "        lifting_off = ((c[:,:,0]==0) * (self.c_prev == 1)).unsqueeze(-1)  \n",
    "        self.p0 = (self.p_sim_prev * lifting_off) + (self.p0 * ~lifting_off)  \n",
    "\n",
    "        # Retrieve p2 : Retrieve the index of the touch down in the contact sequence : First Non-zero Index : shape(batch_size, num_legs)\n",
    "        # Set the last value of c as ONE to avoid the case of only 0 in the contact sequence, wich return the first element (make more sense to retrun the last)\n",
    "        # With the touch_down index, retrieve the touch down foot position : p2\n",
    "        # Idx shape : (batch_size, num_legs) -> must transform to (batch_size, num_legs, 3, 1) to retrieve position from p of shape (batch_size, num_legs, 3, time_horizon)\n",
    "        # p2 shape (batch_size, num_legs, 3) \n",
    "        c[:,:,-1] = 1 # TODO Does it modify c also outside this function ?\n",
    "        first_non_zero_indx = torch.argmax((c!=0).float(), dim=-1)\n",
    "        p2 = torch.gather(p, -1, first_non_zero_indx.unsqueeze(-1).expand(-1,-1,3).unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        # Retrieve p1 : (x,y) position are define as the middle point between p0 and p1 (lift-off and touch-down). z is heuristcally define\n",
    "        # p1 shape (batch_size, num_legs, 3)\n",
    "        p1 = (self.p0[:,:,:2] + p2[:,:,:2]) / 2     # p1(x,y) is in the middle of p0 and p2\n",
    "        p1 = torch.cat((p1, step_height*torch.ones_like(p1[:,:,:1])), dim=2) # Append a third dimension z : defined as step_height\n",
    "\n",
    "        # Step 2. Compute the parameters for the interpolation\n",
    "\n",
    "        # Swing time : reset if lifting off, then increment by one time step (outer loop)  (squeeze lifting_off : (batch_size, num_legs, 1)->(batch_size, num_legs))\n",
    "        # then compute t in [0, Delta_t/2], which would be use for the spline interpolation\n",
    "        # t & swing_time shape (batch_size, num_legs)\n",
    "        self.swing_time = (self.swing_time * ~lifting_off.squeeze(-1)) + self._dt_out\n",
    "        t = self.swing_time % (half_swing_period.abs() + 1e-10)  # Swing time (half) : add small numerical value to avoid nan when % 0\n",
    "\n",
    "        # Compute the a,b,c,d polynimial coefficient for the cubic interpolation S(t) = a*t^3 + b*t^2 + c*t + d\n",
    "        # If swing_time < swing period/2 -> S_0(t) (ie. first interpolation), otherwise -> S_1(t - delta_t/2) (ie. second interpolation)\n",
    "        # cp_x shape (batch_size, num_legs, 3)\n",
    "        is_S0 = (self.swing_time <=  half_swing_period).unsqueeze(-1).expand(*[-1] * len(self.swing_time.shape), 3)  # shape (batch_size, num_legs, 3)\n",
    "        cp1 = (self.p0 * is_S0)                                         + (p1 * ~is_S0)\n",
    "        cp2 = (self.p0 * is_S0)                                         + (torch.cat((p2[:,:,:2], p1[:,:,2:]), dim=2)* ~is_S0)\n",
    "        cp3 = (torch.cat((self.p0[:,:,:2], p1[:,:,2:]), dim=2) * is_S0) + (p2 * ~is_S0)\n",
    "        cp4 = (p1 * is_S0)                                              + (p2 * ~is_S0)\n",
    "\n",
    "        # Step 3. Prepare parameters to compute interpolation trajectory in one operation -> matrix multiplication\n",
    "        \n",
    "        # Generate the time trajectory t -> [t, t + dt, t+ 2*dt,...]\n",
    "        # time_fac, t : shape(batch_size, num_legs) -> unsqueezed(-1) -> Shape (batch_size, num_legs, 1)\n",
    "        # (arrange = [0,1,2,...])*dt.unsqueeze(0).unsqueeze(-1)       -> Shape (1, 1, decimation)\n",
    "        # time traj : Shape (batch_size, num_legs, decimation)\n",
    "        t_traj = (time_fac*t).unsqueeze(-1) + (torch.arange(self._decimation, device=self._device)*self._dt_in).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        # Prepare cp_x to be mutltiplied by the time traj :  shape(batch_size, num_leg, 3) -> (batch_size, num_leg, 3, 1)\n",
    "        cp1 = cp1.unsqueeze(-1)\n",
    "        cp2 = cp2.unsqueeze(-1)\n",
    "        cp3 = cp3.unsqueeze(-1)\n",
    "        cp4 = cp4.unsqueeze(-1)\n",
    "\n",
    "        # Prepare time traj to be multplied by cp_x : shape(batch_size, num_leg, decimation) -> (batch_size, num_leg, 1, decimation)\n",
    "        t_traj = t_traj.unsqueeze(2)\n",
    "\n",
    "\n",
    "        # Step 4. Compute the interpolation trajectory\n",
    "        # shape (batch_size, num_legs, 3, decimation)\n",
    "        desired_foot_pos_traj = cp1*(1 - t_traj)**3 + 3*cp2*(t_traj)*(1 - t_traj)**2 + 3*cp3*((t_traj)**2)*(1 - t_traj) + cp4*(t_traj)**3\n",
    "        desired_foot_vel_traj = 3*(cp2 - cp1)*(1 - t_traj)**2 + 6*(cp3 - cp2)*(1 - t_traj)*(t_traj) + 3*(cp4 - cp3)*(t_traj)**2\n",
    "        desired_foot_acc_traj = 6*(1 - t_traj) * (cp3 - 2*cp2 + cp1) + 6 * (t_traj) * (cp4 - 2*cp3 + cp2)\n",
    "\n",
    "        # shape (batch_size, num_legs, 9, decimation) (9 = xyz_pos, xzy_vel, xyz_acc)\n",
    "        pt = torch.cat((desired_foot_pos_traj, desired_foot_vel_traj, desired_foot_acc_traj), dim=2)\n",
    "\n",
    "        if(pt.isnan().any()):\n",
    "            print('alo')\n",
    "        return pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(10).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4, 3, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "J = torch.randn(10,4,3,3)\n",
    "R = torch.randn(10,3,3)\n",
    "\n",
    "torch.matmul(J,R.unsqueeze(1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Riccardo's Jacobian\n",
    "        from omni.isaac.orbit.managers import SceneEntityCfg\n",
    "\n",
    "        fl_entity_cfg = SceneEntityCfg(\"robot\", joint_names=[\"FL.*\"], body_names=[\"FL_foot\"])\n",
    "        fl_entity_cfg.resolve(self._env.scene)\n",
    "        fl_jacobi_idx = fl_entity_cfg.body_ids[0]\n",
    "        fl_jacobian = self._asset.root_physx_view.get_jacobians()[:, fl_jacobi_idx, :, np.array(fl_entity_cfg.joint_ids) + 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Retrieve p2 : Retrieve the index of the touch down in the contact sequence : First Non-zero Index : shape(batch_size, num_legs)\n",
    "        # Set the last value of c as ONE to avoid the case of only 0 in the contact sequence, wich return the first element (make more sense to retrun the last)\n",
    "        # With the touch_down index, retrieve the touch down foot position : p2\n",
    "        # Idx shape : (batch_size, num_legs) -> must transform to (batch_size, num_legs, 3, 1) to retrieve position from p of shape (batch_size, num_legs, 3, time_horizon)\n",
    "        # p2 shape (batch_size, num_legs, 3) \n",
    "        c[:,:,-1] = 1 # TODO Does it modify c also outside this function ?\n",
    "        first_non_zero_indx = torch.argmax((c!=0).float(), dim=-1)\n",
    "        p2 = torch.gather(p_b, -1, first_non_zero_indx.unsqueeze(-1).expand(-1,-1,3).unsqueeze(-1)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def swing_trajectory_generator(self, p_lw: torch.Tensor, c: torch.Tensor, f: torch.Tensor, d: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Given feet position sequence and contact sequence -> compute swing trajectories by fitting a cubic spline between\n",
    "        the lift-off and the touch down define in the contact sequence. \n",
    "        - Swing frequency and duty cycle are used to compute the swing period\n",
    "        - A middle point is used for the interpolation : which is heuristically defined. It defines the step height\n",
    "        - p1 (middle point) and p2 (touch-down) are updated each time, while p0 is conserved (always the same lift off position)\n",
    "        Note :\n",
    "            The variable are in the 'local' world frame _wl. This notation is introduced to avoid confusion with the 'global' world frame, where all the batches coexists.\n",
    "        \n",
    "        Args:\n",
    "            - p_lw (trch.Tensor): Foot touch down postion in _lw        of shape(batch_size, num_legs, 3)\n",
    "            - c   (torch.Tensor): Foot contact sequence                 of shape(batch_size, num_legs, time_horizon)\n",
    "            - f   (torch.Tensor): Leg frequency                         of shape(batch_size, num_legs)\n",
    "            - d   (torch.Tensor): Stepping duty cycle                   of shape(batch_size, num_legs)\n",
    "\n",
    "        Returns:\n",
    "            - pt_lw (tch.Tensor): Desired Swing Leg traj. in _lw frame  of shape(batch_size, num_legs, 9, decimation)   (9 = xyz_pos, xzy_vel, xyz_acc)\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 0. Define and Compute usefull variables\n",
    "\n",
    "        # Heuristic TODO Save that on the right place, could also be a RL variable\n",
    "        step_height = 0.05\n",
    "\n",
    "        # Time during wich the leg is in swing.(add small numerical value to denominator to avoid division by 0)\n",
    "        # Shape (batch_size, num_legs)\n",
    "        swing_period = ((1-d) / (f.abs()+1e-10))\n",
    "        half_swing_period = swing_period / 2\n",
    "        time_fac = 1 / ((swing_period.abs()+1e-10) / 2) #bezier_time_factor\n",
    "\n",
    "\n",
    "        # Step 1. Retrieve the three interpolation points : p0, p1, p2 (lift-off, middle point, touch down)\n",
    "\n",
    "        # Retrieve p0 : If c(0)=0 and c(-1)=1 : The leg lift-off -> p0 = p(-1) (value from simulation : the last value where c=1)\n",
    "        # Update only the p0 that are new lift off positions (unsqueeze lifting off -> shape(batc_size, num_legs, 1) to make it compatible for multiplication with p shape)\n",
    "        # p0 shape (batch_size, num_legs, 3) \n",
    "        # TODO Improve this and debug using viz\n",
    "        lifting_off = ((c[:,:,0]==0) * (self.c_prev == 1)).unsqueeze(-1)  \n",
    "        self.p0 = (self.p_lw_sim_prev * lifting_off) + (self.p0 * ~lifting_off)  \n",
    "\n",
    "        # Retrieve p2 : this is simply the foot touch down prior given as input\n",
    "        # p2 shape (batch_size, num_legs, 3) \n",
    "        p2 = p_lw \n",
    "\n",
    "        # Retrieve p1 : (x,y) position are define as the middle point between p0 and p1 (lift-off and touch-down). z is heuristcally define\n",
    "        # p1 shape (batch_size, num_legs, 3)\n",
    "        # TODO Not only choose height as step heigh but use +the terrain height or +the feet height at touch down\n",
    "        p1 = (self.p0[:,:,:2] + p2[:,:,:2]) / 2     # p1(x,y) is in the middle of p0 and p2\n",
    "        p1 = torch.cat((p1, step_height*torch.ones_like(p1[:,:,:1])), dim=2) # Append a third dimension z : defined as step_height\n",
    "\n",
    "        # Step 2. Compute the parameters for the interpolation\n",
    "\n",
    "        # Swing time : reset if lifting off, then increment by one time step (outer loop)  (squeeze lifting_off : (batch_size, num_legs, 1)->(batch_size, num_legs))\n",
    "        # then compute t in [0, Delta_t/2], which would be use for the spline interpolation\n",
    "        # t & swing_time shape (batch_size, num_legs)\n",
    "        self.swing_time = (self.swing_time * ~lifting_off.squeeze(-1)) + self._dt_out\n",
    "        t = self.swing_time % (half_swing_period.abs() + 1e-10)  # Swing time (half) : add small numerical value to avoid nan when % 0\n",
    "\n",
    "        # Compute the a,b,c,d polynimial coefficient for the cubic interpolation S(t) = a*t^3 + b*t^2 + c*t + d\n",
    "        # If swing_time < swing period/2 -> S_0(t) (ie. first interpolation), otherwise -> S_1(t - delta_t/2) (ie. second interpolation)\n",
    "        # cp_x shape (batch_size, num_legs, 3)\n",
    "        is_S0 = (self.swing_time <=  half_swing_period).unsqueeze(-1).expand(*[-1] * len(self.swing_time.shape), 3)  # shape (batch_size, num_legs, 3)\n",
    "        cp1 = (self.p0 * is_S0)                                         + (p1 * ~is_S0)\n",
    "        cp2 = (self.p0 * is_S0)                                         + (torch.cat((p2[:,:,:2], p1[:,:,2:]), dim=2)* ~is_S0)\n",
    "        cp3 = (torch.cat((self.p0[:,:,:2], p1[:,:,2:]), dim=2) * is_S0) + (p2 * ~is_S0)\n",
    "        cp4 = (p1 * is_S0)                                              + (p2 * ~is_S0)\n",
    "\n",
    "        # Step 3. Prepare parameters to compute interpolation trajectory in one operation -> matrix multiplication\n",
    "        \n",
    "        # Generate the time trajectory t -> [t, t + dt, t+ 2*dt,...]\n",
    "        # time_fac, t : shape(batch_size, num_legs) -> unsqueezed(-1) -> Shape (batch_size, num_legs, 1)\n",
    "        # (arrange = [0,1,2,...])*dt.unsqueeze(0).unsqueeze(-1)       -> Shape (1, 1, decimation)\n",
    "        # time traj : Shape (batch_size, num_legs, decimation)\n",
    "        t_traj = (time_fac*t).unsqueeze(-1) + (torch.arange(self._decimation, device=self._device)*self._dt_in).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        # Prepare cp_x to be mutltiplied by the time traj :  shape(batch_size, num_leg, 3) -> (batch_size, num_leg, 3, 1)\n",
    "        cp1 = cp1.unsqueeze(-1)\n",
    "        cp2 = cp2.unsqueeze(-1)\n",
    "        cp3 = cp3.unsqueeze(-1)\n",
    "        cp4 = cp4.unsqueeze(-1)\n",
    "\n",
    "        # Prepare time traj to be multplied by cp_x : shape(batch_size, num_leg, decimation) -> (batch_size, num_leg, 1, decimation)\n",
    "        t_traj = t_traj.unsqueeze(2)\n",
    "\n",
    "\n",
    "        # Step 4. Compute the interpolation trajectory\n",
    "        # shape (batch_size, num_legs, 3, decimation)\n",
    "        desired_foot_pos_traj = cp1*(1 - t_traj)**3 + 3*cp2*(t_traj)*(1 - t_traj)**2 + 3*cp3*((t_traj)**2)*(1 - t_traj) + cp4*(t_traj)**3\n",
    "        desired_foot_vel_traj = 3*(cp2 - cp1)*(1 - t_traj)**2 + 6*(cp3 - cp2)*(1 - t_traj)*(t_traj) + 3*(cp4 - cp3)*(t_traj)**2\n",
    "        desired_foot_acc_traj = 6*(1 - t_traj) * (cp3 - 2*cp2 + cp1) + 6 * (t_traj) * (cp4 - 2*cp3 + cp2)\n",
    "\n",
    "        # shape (batch_size, num_legs, 9, decimation) (9 = xyz_pos, xzy_vel, xyz_acc)\n",
    "        pt_b = torch.cat((desired_foot_pos_traj, desired_foot_vel_traj, desired_foot_acc_traj), dim=2)\n",
    "\n",
    "        if(pt_b.isnan().any()):\n",
    "            print('alo')\n",
    "        return pt_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def get_robot_state(self):\n",
    "        \"\"\" Retrieve the Robot states from the simulator\n",
    "\n",
    "        Return :\n",
    "            - p   (torch.Tensor): Feet Position  (latest from sim)      of shape(batch_size, num_legs, 3)\n",
    "            - p_dot (tch.Tensor): Feet velocity  (latest from sim)      of shape(batch_size, num_legs, 3)\n",
    "            - q_dot (tch.Tensor): Joint velocity (latest from sim)      of shape(batch_size, num_legs, num_joints_per_leg)\n",
    "            - jacobian_w  (Tsor): Jacobian -> joint frame to foot frame of shape(batch_size, num_legs, 3, num_joints_per_leg)\n",
    "            - jacobian_dot_w (T): Jacobian derivative (forward euler)   of shape(batch_size, num_legs, 3, num_joints_per_leg)\n",
    "            - mass_matrix (Tsor): Mass Matrix in joint space            of shape(batch_size, num_legs, num_joints_per_leg, num_joints_per_leg)\n",
    "            - h   (torch.Tensor): C(q,q_dot) + G(q) (corr. and grav F.) of shape(batch_size, num_legs, num_joints_per_leg)\n",
    "        \"\"\"\n",
    "\n",
    "        # Retrieve robot base position and orientation in order to compute world->base frame transformation\n",
    "        robot_pos_w = self._asset.data.root_pos_w       # shape (batch_size, 3) (xyz)\n",
    "        robot_orientation_w = self._asset.data.root_quat_w # shape (batch_size, 4) (quaternions)\n",
    "        robot_vel_w = self._asset.data.root_lin_vel_w\n",
    "        # robot_ang_vel_w = self._asset.data.root_ang_vel_w\n",
    "\n",
    "        # Retrieve Feet position in world frame : [num_instances, num_bodies, 3] select right indexes to get \n",
    "        # shape(batch_size, num_legs, 3)\n",
    "        # Finally apply frame transformation to get feet position in body frame\n",
    "        p_w = self._asset.data.body_pos_w[:, self._foot_idx,:]\n",
    "        p_orientation_w = self._asset.data.body_quat_w[:, self._foot_idx,:]\n",
    "        p_b_0, _ = math_utils.subtract_frame_transforms(robot_pos_w, robot_orientation_w, p_w[:,0,:], p_orientation_w[:,0,:])\n",
    "        p_b_1, _ = math_utils.subtract_frame_transforms(robot_pos_w, robot_orientation_w, p_w[:,1,:], p_orientation_w[:,1,:])\n",
    "        p_b_2, _ = math_utils.subtract_frame_transforms(robot_pos_w, robot_orientation_w, p_w[:,2,:], p_orientation_w[:,2,:])\n",
    "        p_b_3, _ = math_utils.subtract_frame_transforms(robot_pos_w, robot_orientation_w, p_w[:,3,:], p_orientation_w[:,3,:])\n",
    "        p_b = torch.cat((p_b_0.unsqueeze(1), p_b_1.unsqueeze(1), p_b_2.unsqueeze(1), p_b_3.unsqueeze(1)), dim=1)\n",
    "\n",
    "        # Retrieve Feet velocity in world frame : [num_instances, num_bodies, 3] select right indexes to get \n",
    "        # shape(batch_size, num_legs, 3)\n",
    "        # Finally apply frame transformation to get feet position in body frame\n",
    "        p_dot_w = self._asset.data.body_lin_vel_w[:, self._foot_idx,:]\n",
    "        # p_dot_orientation_w = self._asset.data.body_ang_vel_w[:, self._foot_idx, :]\n",
    "        p_dot_b_0, _ = math_utils.subtract_frame_transforms(robot_vel_w, robot_orientation_w, p_dot_w[:,0,:], p_orientation_w[:,0,:])\n",
    "        p_dot_b_1, _ = math_utils.subtract_frame_transforms(robot_vel_w, robot_orientation_w, p_dot_w[:,1,:], p_orientation_w[:,1,:])\n",
    "        p_dot_b_2, _ = math_utils.subtract_frame_transforms(robot_vel_w, robot_orientation_w, p_dot_w[:,2,:], p_orientation_w[:,2,:])\n",
    "        p_dot_b_3, _ = math_utils.subtract_frame_transforms(robot_vel_w, robot_orientation_w, p_dot_w[:,3,:], p_orientation_w[:,3,:])\n",
    "        p_dot_b = torch.cat((p_dot_b_0.unsqueeze(1), p_dot_b_1.unsqueeze(1), p_dot_b_2.unsqueeze(1), p_dot_b_3.unsqueeze(1)), dim=1)\n",
    "\n",
    "        # Retrieve Joint velocities [num_instances, num_joints] -> reorganise the view and permute to get the\n",
    "        # shape(batch_size, num_legs, num_joints_per_leg) : This is in joint space, no transformation required\n",
    "        q_dot = self._asset.data.joint_vel.view(-1,self._num_joints_per_leg,self._num_legs).permute(0,2,1)\n",
    "\n",
    "        # Retrieve Jacobian from sim  shape(batch_size, num_legs, 3, num_joints_per_leg) -> see method for implementation\n",
    "        jacobian_w, jacobian_b = self.get_jacobian()\n",
    "\n",
    "        # Compute jacobian derivative, using forward euler. shape(batch_size, num_legs, 3, num_joints_per_leg)\n",
    "        jacobian_dot_w = ((jacobian_w - self.jacobian_prev_w) / self._env.physics_dt)\n",
    "\n",
    "        # Save jacobian for next iteration : required to compute jacobian derivative shape(batch_size, num_legs, 3, num_joints_per_leg)\n",
    "        self.jacobian_prev_w = jacobian_w\n",
    "        \n",
    "        # Retrieve the mass Matrix\n",
    "        # Shape is (batch_size, num_joints, num_joints) (ie. 144 element), we have to extract num leg sub matrices from that to have \n",
    "        # shape (batch_size, num_leg, num_joints_per_leg, num_joints_per_leg) (ie. 36 elements)\n",
    "        # This is done with complex indexing operations\n",
    "        # mass_matrix_full = self._asset.root_physx_view.get_mass_matrices()\n",
    "        # mass_matrix_FL = mass_matrix_full[:,[0,4,8],:][:, [0,4,8]]\n",
    "        joints_idx_tensor = torch.Tensor(self._joints_idx).unsqueeze(2).unsqueeze(3).long() # long to use it to access indexes -> float trow an error\n",
    "        mass_matrix = self._asset.root_physx_view.get_mass_matrices()[:, joints_idx_tensor, joints_idx_tensor.transpose(1,2)].squeeze(-1)\n",
    "        \n",
    "        # Retrieve Corriolis, centrifugial and gravitationnal term\n",
    "        # get_coriolis_and_centrifugal_forces -> (batch_size, num_joints)\n",
    "        # get_generalized_gravity_forces -> (batch_size, num_joints)\n",
    "        # Reshape and tranpose to get the correct shape in correct joint order-> (batch_size, num_legs, num_joints_per_leg)\n",
    "        h = (self._asset.root_physx_view.get_coriolis_and_centrifugal_forces() + self._asset.root_physx_view.get_generalized_gravity_forces()).view(self.num_envs, self._num_joints_per_leg, self._num_legs).permute(0,2,1)\n",
    "\n",
    "        return p_b, p_dot_b, q_dot, jacobian_w, jacobian_dot_w, mass_matrix, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True,  True, False],\n",
      "         [ True, False, False],\n",
      "         [ True, False,  True],\n",
      "         [ True,  True,  True]],\n",
      "\n",
      "        [[False,  True, False],\n",
      "         [ True,  True,  True],\n",
      "         [ True,  True, False],\n",
      "         [ True, False, False]]])\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[ True,  True,  True,  True],\n",
      "        [False,  True,  True,  True]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True],\n",
       "        [False,  True,  True,  True]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "c = torch.empty((2,4,3), dtype=torch.bool).bernoulli(0.5)\n",
    "print(c)\n",
    "print(c.shape)\n",
    "\n",
    "print(c[:,:,0])\n",
    "c[:,:,0]==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "tensor([[ 0,  4,  8],\n",
      "        [ 1,  5,  9],\n",
      "        [ 2,  6, 10],\n",
      "        [ 3,  7, 11]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jac = torch.tensor([[0,4,8],[1,5,9],[2,6,10],[3,7,11]])\n",
    "print(jac.shape)\n",
    "print(jac)\n",
    "\n",
    "jac.permute(1,0).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "f = torch.tensor([[0,4,8],[1,5,9],[2,6,10],[3,7,11]])\n",
    "print(f.shape)\n",
    "\n",
    "f.shape[1:].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "\n",
      "tensor([[0.5000, 0.0000, 0.5000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.0000, 0.5000, 0.0000]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "phase = torch.tensor([[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0], [0.0,0.0,0.0,0.0]])\n",
    "print(phase)\n",
    "print()\n",
    "\n",
    "env_ids = [0,2]\n",
    "\n",
    "phase[env_ids,:] = torch.zeros_like(phase)[env_ids,:]\n",
    "print(phase)\n",
    "print()\n",
    "\n",
    "# (phase[env_ids,:])[:,(0,2)] = 0.5 # Init phase [0.5, 0, 0.5, 0]\n",
    "phase[env_ids,0] = 0.5 # Init phase [0.5, 0, 0.5, 0]\n",
    "phase[env_ids,2] = 0.5 # Init phase [0.5, 0, 0.5, 0]\n",
    "print(phase)\n",
    "print()\n",
    "\n",
    "# print(phase[env_ids,:2])\n",
    "# print()\n",
    "\n",
    "# phase[:,(0,2)] = 0.5\n",
    "# print(phase)\n",
    "# print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "        0.9000, 1.0000, 1.1000])\n",
      "tensor([ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "        False, False])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.2000, 0.4000, 0.6000, 0.8000, 1.0000, 0.2000, 0.4000, 0.6000,\n",
       "        0.8000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "T = torch.tensor([[0,4,8],[1,5,9], [2,6,10],[3,7,11]])\n",
    "T.permute(1,0).reshape(12)\n",
    "\n",
    "phase = torch.arange(start=0, end=1.11, step=0.1)\n",
    "print(phase)\n",
    "\n",
    "is_S0 = phase <= 0.5\n",
    "\n",
    "print(is_S0)\n",
    "\n",
    "(2*phase - 1*(~is_S0)).clamp(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1, 22])\n",
      "torch.Size([1, 1, 1, 22])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "full_phase_traj = torch.cat((torch.arange(start=0, end=1.01, step=0.1), torch.arange(start=0, end=1.01, step=0.1))).unsqueeze(0).unsqueeze(1).unsqueeze(2)\n",
    "print(full_phase_traj.shape)\n",
    "is_S0 = (torch.arange(start=0, end=22, step=1) < 11).unsqueeze(0).unsqueeze(1).unsqueeze(2)\n",
    "print(is_S0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.0000, -1.0000,  0.0300],\n",
      "         [ 2.0000, -2.0000,  0.0300],\n",
      "         [ 3.0000, -3.0000,  0.0300],\n",
      "         [ 4.0000, -4.0000,  0.0300]],\n",
      "\n",
      "        [[ 1.0000, -1.0000,  0.0300],\n",
      "         [ 2.0000, -2.0000,  0.0300],\n",
      "         [ 3.0000, -3.0000,  0.0300],\n",
      "         [ 4.0000, -4.0000,  0.0300]],\n",
      "\n",
      "        [[ 1.0000, -1.0000,  0.0300],\n",
      "         [ 2.0000, -2.0000,  0.0300],\n",
      "         [ 3.0000, -3.0000,  0.0300],\n",
      "         [ 4.0000, -4.0000,  0.0300]],\n",
      "\n",
      "        [[ 1.0000, -1.0000,  0.0300],\n",
      "         [ 2.0000, -2.0000,  0.0300],\n",
      "         [ 3.0000, -3.0000,  0.0300],\n",
      "         [ 4.0000, -4.0000,  0.0300]]])\n",
      "tensor([[[ 2.0000, -2.0000,  0.0300],\n",
      "         [ 3.0000, -3.0000,  0.0300],\n",
      "         [ 4.0000, -4.0000,  0.0300],\n",
      "         [ 1.0000, -1.0000,  0.0300]],\n",
      "\n",
      "        [[ 2.0000, -2.0000,  0.0300],\n",
      "         [ 3.0000, -3.0000,  0.0300],\n",
      "         [ 4.0000, -4.0000,  0.0300],\n",
      "         [ 1.0000, -1.0000,  0.0300]],\n",
      "\n",
      "        [[ 2.0000, -2.0000,  0.0300],\n",
      "         [ 3.0000, -3.0000,  0.0300],\n",
      "         [ 4.0000, -4.0000,  0.0300],\n",
      "         [ 1.0000, -1.0000,  0.0300]],\n",
      "\n",
      "        [[ 2.0000, -2.0000,  0.0300],\n",
      "         [ 3.0000, -3.0000,  0.0300],\n",
      "         [ 4.0000, -4.0000,  0.0300],\n",
      "         [ 1.0000, -1.0000,  0.0300]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "FOOT_OFFSET = 0.03\n",
    "# Find the corner points of the polygon - provide big values that will be clipped to corresponding bound\n",
    "# p shape(num_corners, 3)\n",
    "p = torch.tensor([[1,-1,FOOT_OFFSET],[2,-2,FOOT_OFFSET],[3,-3,FOOT_OFFSET],[4,-4,FOOT_OFFSET]])\n",
    "\n",
    "# Reshape p to be passed to transform_p_from_rl_to_lw -> (num_corner, num_legs, 3, 1)\n",
    "p = p.unsqueeze(1).expand(4,4,3)\n",
    "\n",
    "p = p.permute(1,0,2)\n",
    "# p[:,:,:] = 2*p[:,:,:]\n",
    "\n",
    "print(p)\n",
    "\n",
    "print(p.roll(-1,dims=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x754e7ac50580>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "T = torch.tensor([[0,1,2,3,4,5,6,7,8],[2,7,11,4,5,6,1,2,3]])\n",
    "\n",
    "bound = [2.0,6.0]\n",
    "T.clamp(bound[0], bound[1])\n",
    "\n",
    "plt.plot(T[0,:])\n",
    "\n",
    "plt.plot((T - T.clamp(bound[0], bound[1]))[0,:])\n",
    "plt.plot(-torch.abs((T - T.clamp(bound[0], bound[1]))[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 56])\n",
      "56\n",
      "\n",
      "4\n",
      "torch.Size([64, 4])\n",
      "4\n",
      "torch.Size([64, 4])\n",
      "12\n",
      "torch.Size([64, 4, 3, 1])\n",
      "36\n",
      "torch.Size([64, 4, 3, 3])\n",
      "\n",
      "4\n",
      "torch.Size([64, 4])\n",
      "4\n",
      "torch.Size([64, 4])\n",
      "12\n",
      "torch.Size([64, 4, 3, 1])\n",
      "36\n",
      "torch.Size([64, 4, 3, 3])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_envs = 64\n",
    "_num_legs = 4\n",
    "_number_predict_step = 1\n",
    "_prevision_horizon = 3\n",
    "device = 'cpu'\n",
    "\n",
    "f = 1*torch.ones(num_envs, _num_legs, device=device)\n",
    "d = 0.55*torch.ones(num_envs, _num_legs, device=device)\n",
    "p_lw = torch.zeros(num_envs, _num_legs, 3, _number_predict_step, device=device)\n",
    "p_rl = torch.zeros(num_envs, _num_legs, 3, _number_predict_step, device=device) # Used to compute penalty term\n",
    "F_lw = torch.zeros(num_envs, _num_legs, 3, _prevision_horizon, device=device)\n",
    "z = [f, d, p_lw, F_lw]\n",
    "\n",
    "actions = torch.randn(num_envs, sum(variable.shape[1:].numel() for variable in z), device=device)\n",
    "print(actions.shape)\n",
    "\n",
    "print(sum(variable.shape[1:].numel() for variable in z)) \n",
    "print()\n",
    "\n",
    "for variable in z:\n",
    "    print(variable.shape[1:].numel())\n",
    "    print(variable.shape)\n",
    "print()\n",
    "\n",
    "f = actions[:, 0 : f.shape[1:].numel()]\n",
    "d = actions[:, f.shape[1:].numel() : f.shape[1:].numel() + d.shape[1:].numel()]\n",
    "p_lw = actions[:, f.shape[1:].numel() + d.shape[1:].numel() : f.shape[1:].numel() + d.shape[1:].numel() + p_lw.shape[1:].numel()].reshape_as(p_lw)\n",
    "F_lw = actions[:, f.shape[1:].numel() + d.shape[1:].numel() + p_lw.shape[1:].numel() : f.shape[1:].numel() + d.shape[1:].numel() + p_lw.shape[1:].numel() + F_lw.shape[1:].numel()].reshape_as(F_lw)\n",
    "\n",
    "z = [f, d, p_lw, F_lw]\n",
    "for variable in z:\n",
    "    print(variable.shape[1:].numel())\n",
    "    print(variable.shape)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.3365e-01,  9.6838e-01, -1.8004e+00,  3.1203e+00,  1.1875e+00],\n",
       "         [ 1.1013e+00, -5.6833e+00,  1.5057e+00, -3.8912e+00, -2.6385e+00],\n",
       "         [ 9.3424e-01,  9.9264e-01, -2.3082e+00, -1.3657e+00,  6.7267e-01]],\n",
       "\n",
       "        [[-1.9442e-01, -3.4300e-01,  7.8670e-02, -5.7622e-01,  9.1210e-02],\n",
       "         [-5.3612e-01, -2.1279e-01, -2.2476e-01, -1.4254e-01, -2.1596e-03],\n",
       "         [ 3.2512e-02,  1.7837e-01,  2.4883e-01, -2.8350e-01,  5.2160e-01]],\n",
       "\n",
       "        [[-4.2796e-01,  4.9321e+00,  3.3061e+00,  1.8717e+00, -9.5266e-01],\n",
       "         [-1.7079e+00, -7.3135e-01, -1.7175e+00,  1.8330e+00, -2.6230e+00],\n",
       "         [-2.7789e+00,  1.1727e+00,  3.2427e+00, -1.6299e+00,  1.7539e+00]],\n",
       "\n",
       "        [[ 2.5066e+00, -6.2748e+00,  5.5286e+00, -1.2821e+01,  1.4550e-02],\n",
       "         [-5.8101e-01, -1.6196e-01,  1.8136e+00,  8.8238e+00,  1.9647e-01],\n",
       "         [-1.8229e+00,  2.2025e+00,  4.6450e+00,  6.7667e+00,  2.1184e+00]],\n",
       "\n",
       "        [[ 8.8243e-01,  1.2052e-01, -9.6759e-01, -1.8618e-01, -2.2502e-01],\n",
       "         [ 6.0421e-01, -2.1540e-01, -5.2019e-01, -5.1792e-01, -7.7056e-02],\n",
       "         [-3.2260e-02,  5.6294e-01,  3.8231e-01,  5.1246e-01,  3.5319e-02]],\n",
       "\n",
       "        [[ 3.1145e+00, -6.3850e+00,  6.7649e+00,  4.2947e+00,  1.6592e+00],\n",
       "         [-7.5955e+00, -2.0736e-01,  1.2602e+00, -3.2641e+00, -3.5996e+00],\n",
       "         [ 2.0710e+00, -5.1184e+00, -5.2834e+00, -2.1745e+00, -5.7312e-01]],\n",
       "\n",
       "        [[-3.9322e-01, -8.2752e-02,  7.0897e-02,  8.3229e-02,  2.9297e-01],\n",
       "         [-5.1942e-01,  4.4809e-01,  2.2333e-01,  5.9992e-01,  7.0025e-01],\n",
       "         [ 1.2154e-01, -3.6068e-01, -1.2098e-01, -3.0937e-01, -4.3617e-01]],\n",
       "\n",
       "        [[-4.5236e+00,  1.4031e+00,  3.2954e+00,  3.1699e-01,  4.0366e+00],\n",
       "         [-3.6423e+00, -1.7729e+00, -1.6050e-01, -4.3464e+00,  3.1035e+00],\n",
       "         [-1.7457e+00, -1.5222e+00,  3.2360e-01,  1.0012e+00,  1.5124e+00]],\n",
       "\n",
       "        [[-5.4073e-03,  9.4162e-03, -5.6676e-03,  1.5927e-02, -1.6696e-02],\n",
       "         [-4.1429e-03, -1.5816e-02,  4.4257e-03, -1.7841e-04, -3.6205e-03],\n",
       "         [-8.7526e-03, -1.2371e-03, -1.7579e-03, -2.1407e-03,  2.0334e-02]],\n",
       "\n",
       "        [[-1.6593e+00, -4.1087e-01,  5.1033e-01, -1.0704e+00, -3.6579e-01],\n",
       "         [-3.1808e+00, -4.3288e+00, -4.5706e+00, -3.3593e+00, -1.7024e+00],\n",
       "         [-1.6746e-01,  7.2090e-01,  5.6026e-01, -3.3042e+00,  3.5694e+00]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.randn(10,4)\n",
    "b = torch.sum(a, dim=1)\n",
    "c = torch.randn(10,3,5)\n",
    "print(b.shape)\n",
    "print(c.shape)\n",
    "b.unsqueeze(-1).unsqueeze(-1)*c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example dimensions\n",
    "b = 2\n",
    "n = 3\n",
    "\n",
    "# Example tensors p and p_prev of dimension (b, n, 3)\n",
    "p = torch.randn(b, n, 3)\n",
    "p_prev = torch.randn(b, n, 3)\n",
    "\n",
    "# Calculate the squared difference and sum along the last dimension\n",
    "squared_difference_sum = torch.sum(torch.square(p - p_prev), dim=-1)\n",
    "\n",
    "# Verify the output shape\n",
    "print(\"Output shape:\", squared_difference_sum.shape)  # Output: (b,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 4])\n",
      "tensor([0.2023, 0.0271, 0.8369, 0.7345, 0.4740, 0.3796, 0.0417, 0.9282, 0.0021,\n",
      "        0.3301, 0.4110, 0.2978, 0.8927, 0.6978, 0.5533, 0.6596, 0.4820, 0.2477])\n",
      "tensor([ 4, 15, 11, 17])\n",
      "tensor([0.4740, 0.6596, 0.2978, 0.2477])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example usage\n",
    "batch_size = 10\n",
    "grid_size = 18\n",
    "n = 4\n",
    "\n",
    "# Dummy sensor_data and index tensors\n",
    "sensor_data = torch.rand(batch_size, grid_size, 3)\n",
    "index = torch.randint(0, grid_size, (batch_size, n))\n",
    "\n",
    "# Retrieve data in a single flow using advanced indexing\n",
    "retrieved_data = sensor_data[torch.arange(batch_size).unsqueeze(1), index, 2]\n",
    "\n",
    "print(retrieved_data.shape)  # Output: torch.Size([2, 3, 3])\n",
    "\n",
    "torch.arange(batch_size).unsqueeze(1)\n",
    "index\n",
    "\n",
    "print(sensor_data[2,:,2])\n",
    "print(index[2,:])\n",
    "print(retrieved_data[2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0059])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.empty(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not all are real, there is a nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0,    0],\n",
       "        [3514,   17,    2]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch.distributions.constraints import real\n",
    "\n",
    "a = torch.randn((4096,183,3))\n",
    "\n",
    "a[1,2,:18]\n",
    "\n",
    "a[0,0,0] = torch.nan\n",
    "a[3514, 17, 2] = torch.nan\n",
    "\n",
    "torch.isreal(a).all()\n",
    "\n",
    "(a == a) # Fail for Nan\n",
    "\n",
    "if not real.check(a).all() :\n",
    "    print('not all are real, there is a nan')\n",
    "\n",
    "torch.nonzero(~real.check(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 4, 3])\n",
      "torch.Size([16, 4, 2])\n",
      "10 tensor(10.)\n",
      "10 tensor(10.)\n",
      "-10 tensor(-10.)\n",
      "0 tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "F = torch.randn((16,4,3))\n",
    "\n",
    "print(F[:,:,:].shape)\n",
    "print(F[:,:,:2].shape)\n",
    "\n",
    "F[0,0,0] = 10\n",
    "print('10',F[0,0,0])\n",
    "\n",
    "F = F.clamp(min=0)\n",
    "print('10',F[0,0,0])\n",
    "\n",
    "F[0,0,0] = -10\n",
    "print('-10',F[0,0,0])\n",
    "\n",
    "F = F.clamp(min=0)\n",
    "print('0',F[0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.randint(0, 1, (20,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "# from __future__ import print_function\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataloader import ObservationActionDataset, ChunkedObservationActionDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "# dataset = ChunkedObservationActionDataset('dataset/aliengo_model_based_speed/mcQueenOne/training_data_chunk_*.pt')\n",
    "dataset = ObservationActionDataset('dataset/aliengo_model_based_speed/mcQueenFour/training_data.pt')\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "# Define model\n",
    "class Model1(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Model1, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.lin1 = nn.Linear(input_size, 512)\n",
    "        self.lin2 = nn.Linear(512, 256)\n",
    "        self.lin3 = nn.Linear(256, 128)\n",
    "        self.lin4 = nn.Linear(128, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.lin3(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.lin4(x)\n",
    "        return x\n",
    "\n",
    "# Example model definition\n",
    "input_size = dataset.observations.shape[-1]\n",
    "output_size = dataset.actions.shape[-1]\n",
    "model = Model(input_size, output_size).cuda()\n",
    "\n",
    "print('Input  Size : ', dataset.observations.shape)\n",
    "print('Output Size : ', dataset.actions.shape)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for obs_batch, action_batch in dataloader:\n",
    "        obs_batch, action_batch = obs_batch.cuda(), action_batch.cuda()  # Move to GPU\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(obs_batch)\n",
    "        loss = criterion(outputs, action_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "print('Training completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for the first approach (rand_like and comparison): 0.00036716461181640625\n",
      "Time for the second approach (bernoulli and full_like): 0.0001862049102783203\n",
      "tensor([[[0., 0., 1.,  ..., 0., 1., 0.],\n",
      "         [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 1., 0.,  ..., 0., 0., 1.]],\n",
      "\n",
      "        [[1., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 1., 1.,  ..., 1., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [1., 1., 1.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 0.,  ..., 1., 0., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 1., 1.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0.,  ..., 0., 0., 1.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 1.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "         [1., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [1., 1., 0.,  ..., 1., 0., 1.]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# Assuming B is your input tensor\n",
    "B = torch.empty(1000,1000,100)\n",
    "\n",
    "# Timing the first approach\n",
    "start_time = time.time()\n",
    "A1 = (torch.rand_like(B, device='cuda') > 0.75).float()\n",
    "time1 = time.time() - start_time\n",
    "\n",
    "# Timing the second approach\n",
    "start_time = time.time()\n",
    "A2 = torch.bernoulli(torch.full_like(B, 0.25, device='cuda'))\n",
    "time2 = time.time() - start_time\n",
    "\n",
    "print(\"Time for the first approach (rand_like and comparison):\", time1)\n",
    "print(\"Time for the second approach (bernoulli and full_like):\", time2)\n",
    "\n",
    "print(A1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for clamp_max: 7.128642320632935\n",
      "Time for torch.min: 7.307240962982178\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# Example tensor F and parameters\n",
    "F = torch.rand(1000, 1000, 3, 10, device='cuda')  # Change dimensions as needed\n",
    "alpha = torch.tensor(0.5, device='cuda')\n",
    "F_xy_max = torch.rand(1000,1000,10, device='cuda')\n",
    "iter=10000\n",
    "\n",
    "# Measure time for clamp_max\n",
    "start_time = time.time()\n",
    "for _ in range(iter):\n",
    "    F_x_clipped_1 = F[:,:,0,:].clamp_max(torch.cos(alpha) * F_xy_max)\n",
    "    alpha = torch.rand_like(alpha)\n",
    "time_clamp_max = time.time() - start_time\n",
    "\n",
    "# Measure time for torch.min\n",
    "start_time = time.time()\n",
    "for _ in range(iter):\n",
    "    F_x_clipped_2 = torch.min(F[:,:,0,:], torch.cos(alpha) * F_xy_max)\n",
    "    alpha = torch.rand_like(alpha)\n",
    "time_torch_min = time.time() - start_time\n",
    "\n",
    "print(\"Time for clamp_max:\", time_clamp_max)\n",
    "print(\"Time for torch.min:\", time_torch_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original implementation time: 0.036420583724975586\n",
      "Two-step implementation time: 0.05660271644592285\n",
      "Indexing implementation time: 0.084747314453125\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "# Define the functions\n",
    "def compute_tracking_error_original(target_height, robot_height, upper_bound, lower_bound):\n",
    "    raw_error = target_height - robot_height\n",
    "    tracking_error = torch.where(\n",
    "        robot_height > target_height + upper_bound,\n",
    "        raw_error - upper_bound,\n",
    "        torch.where(\n",
    "            robot_height < target_height - lower_bound,\n",
    "            raw_error + lower_bound,\n",
    "            torch.zeros_like(raw_error)\n",
    "        )\n",
    "    )\n",
    "    return tracking_error\n",
    "\n",
    "def compute_tracking_error_two_step(target_height, robot_height, upper_bound, lower_bound):\n",
    "    raw_error = target_height - robot_height\n",
    "    upper_bound_error = torch.where(robot_height > target_height + upper_bound, raw_error - upper_bound, raw_error)\n",
    "    tracking_error = torch.where(robot_height < target_height - lower_bound, raw_error + lower_bound, upper_bound_error)\n",
    "    tracking_error = torch.where((robot_height >= target_height - lower_bound) & (robot_height <= target_height + upper_bound), torch.zeros_like(tracking_error), tracking_error)\n",
    "    return tracking_error\n",
    "\n",
    "def compute_tracking_error_indexing(target_height, robot_height, upper_bound, lower_bound):\n",
    "    raw_error = target_height - robot_height\n",
    "    tracking_error = raw_error.clone()\n",
    "    tracking_error[robot_height > target_height + upper_bound] -= upper_bound\n",
    "    tracking_error[robot_height < target_height - lower_bound] += lower_bound\n",
    "    within_bounds = (robot_height >= target_height - lower_bound) & (robot_height <= target_height + upper_bound)\n",
    "    tracking_error[within_bounds] = 0\n",
    "    return tracking_error\n",
    "\n",
    "# Initialize tensors\n",
    "batch_size = 4096\n",
    "num=1000\n",
    "target_height = torch.tensor([10.0] * batch_size, device=device)\n",
    "robot_height = torch.tensor([8.0] * batch_size, device=device)\n",
    "upper_bound = torch.tensor([2.0], device=device)\n",
    "lower_bound = torch.tensor([2.0], device=device)\n",
    "\n",
    "# Benchmark original\n",
    "start_time = time.time()\n",
    "for i in range(num): tracking_error1 = compute_tracking_error_original(target_height, robot_height, upper_bound, lower_bound)\n",
    "end_time = time.time()\n",
    "print(\"Original implementation time:\", end_time - start_time)\n",
    "\n",
    "# Benchmark two-step\n",
    "start_time = time.time()\n",
    "for i in range(num): tracking_error2 = compute_tracking_error_two_step(target_height, robot_height, upper_bound, lower_bound)\n",
    "end_time = time.time()\n",
    "print(\"Two-step implementation time:\", end_time - start_time)\n",
    "\n",
    "# Benchmark indexing\n",
    "start_time = time.time()\n",
    "for i in range(num): tracking_error3 = compute_tracking_error_indexing(target_height, robot_height, upper_bound, lower_bound)\n",
    "end_time = time.time()\n",
    "print(\"Indexing implementation time:\", end_time - start_time)\n",
    "\n",
    "print(((tracking_error1-tracking_error2) > 1e-10).any())\n",
    "print(((tracking_error1-tracking_error3) > 1e-10).any())\n",
    "print(((tracking_error2-tracking_error3) > 1e-10).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0000, -0.1000,  0.1000, -0.2000,  0.2000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.0000,  0.0000, -0.1000,  0.1000])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "height_bound = (-0.1, +0.1)\n",
    "\n",
    "target_height = torch.tensor((0.5, 0.5, 0.5, 0.5, 0.5))\n",
    "robot_height_prop = torch.tensor((0.5, 0.4, 0.6, 0.3, 0.7))\n",
    "\n",
    "\n",
    "# Compute the tracking error\n",
    "tracking_error = robot_height_prop - target_height\n",
    "print(tracking_error)\n",
    "# If tolerance bound are provided adjusted error with bounds\n",
    "if height_bound is not None :\n",
    "\n",
    "    tracking_error = torch.where(\n",
    "        robot_height_prop > target_height + height_bound[1],\n",
    "        tracking_error - height_bound[1],\n",
    "        torch.where(\n",
    "            robot_height_prop < target_height + height_bound[0],\n",
    "            tracking_error - height_bound[0],\n",
    "            torch.zeros_like(tracking_error)\n",
    "        )\n",
    "    )\n",
    "\n",
    "tracking_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full pairwise distances time: 0.003140 seconds\n",
      "Optimized upper triangular distances time: 0.001984 seconds\n",
      "Are the results equivalent? True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.9345, 0.5352, 0.7688, 0.4836, 0.6719, 0.7495])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "# Example tensor (batch, legs, 2)\n",
    "batch = 40960\n",
    "legs = 4\n",
    "foot_xy_pos = torch.rand((batch, legs, 2))\n",
    "\n",
    "# Full pairwise distance computation\n",
    "def full_pairwise_distances(foot_xy_pos):\n",
    "    foot_exp1 = foot_xy_pos.unsqueeze(2)\n",
    "    foot_exp2 = foot_xy_pos.unsqueeze(1)\n",
    "    diffs = foot_exp1 - foot_exp2\n",
    "    distances = torch.norm(diffs, dim=-1)\n",
    "    upper_triangle_distances = distances.triu(diagonal=1)\n",
    "    return upper_triangle_distances\n",
    "\n",
    "# Optimized upper triangular distance computation\n",
    "def optimized_upper_triangular_distances(foot_xy_pos):\n",
    "    indices = list(itertools.combinations(range(legs), 2))\n",
    "    distances = []\n",
    "    for i, j in indices:\n",
    "        diff = foot_xy_pos[:, i, :] - foot_xy_pos[:, j, :]\n",
    "        dist = torch.norm(diff, dim=1)\n",
    "        distances.append(dist)\n",
    "    distances = torch.stack(distances, dim=1)\n",
    "    return distances\n",
    "\n",
    "# Benchmarking function\n",
    "def benchmark(func, foot_xy_pos, iterations=1000):\n",
    "    start_time = time.time()\n",
    "    for _ in range(iterations):\n",
    "        result = func(foot_xy_pos)\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time, result\n",
    "\n",
    "# Run benchmarks\n",
    "iterations = 1\n",
    "time_full, result_full = benchmark(full_pairwise_distances, foot_xy_pos, iterations)\n",
    "time_optimized, result_optimized = benchmark(optimized_upper_triangular_distances, foot_xy_pos, iterations)\n",
    "\n",
    "# Print results\n",
    "print(f\"Full pairwise distances time: {time_full:.6f} seconds\")\n",
    "print(f\"Optimized upper triangular distances time: {time_optimized:.6f} seconds\")\n",
    "\n",
    "# Validate that results are equivalent\n",
    "# Extract the upper triangular part (excluding the diagonal) from the full pairwise distances\n",
    "upper_triangle_indices = torch.triu_indices(legs, legs, 1)\n",
    "result_full_upper = result_full[:, upper_triangle_indices[0], upper_triangle_indices[1]]\n",
    "print(f\"Are the results equivalent? {torch.allclose(result_full_upper, result_optimized)}\")\n",
    "\n",
    "result_optimized[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orbit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
